{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "from inflection import underscore\n",
    "\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "import pprint\n"
   ],
   "id": "e68dcd943609255",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def create_duckdb_file(filename):\n",
    "    \"\"\"\n",
    "    Creates a file-based DuckDB database and returns the connection.\n",
    "  \n",
    "    Args:\n",
    "      filename: The name of the DuckDB database file to create.\n",
    "  \n",
    "    Returns:\n",
    "      duckdb.DuckDBPyConnection: The DuckDB connection object.\n",
    "    \"\"\"\n",
    "    conn = duckdb.connect(database=filename)\n",
    "    return conn\n"
   ],
   "id": "e32bd2653ce252d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Example usage:\n",
    "filename = \"biosamples.duckdb\"\n",
    "duckdb_conn = create_duckdb_file(filename)"
   ],
   "id": "e9f0e4e3acc0babb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# MongoDB connection details\n",
    "connection_string = \"mongodb://localhost:27017/\"\n",
    "db_name = \"biosamples\"\n",
    "collection_name = \"biosamples\"\n",
    "\n",
    "# Connect to MongoDB\n",
    "client = MongoClient(connection_string)\n",
    "db = client[db_name]\n",
    "collection = db[collection_name]"
   ],
   "id": "8bc12c0a82a5b072",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Retrieve the first document\n",
    "first_document = collection.find_one()\n"
   ],
   "id": "efcac4a393739da5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # Print the document\n",
    "# pprint.pprint(first_document)"
   ],
   "id": "ae7f62c60dea697b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# df['content'].value_counts()",
   "id": "39749d18f30ade87",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # now processing everything except Description.Comment (antibiograms) and BioSample.Owner.Contacts\n",
    "# paths = [\n",
    "#     \"BioSample\",\n",
    "#     \"BioSample.Attributes.Attribute\",\n",
    "#     \"BioSample.Curation\",\n",
    "#     \"BioSample.Description.Comment.Paragraph\",\n",
    "#     \"BioSample.Description.Organism\",\n",
    "#     \"BioSample.Description.Organism.OrganismName\",\n",
    "#     \"BioSample.Description.Synonym\",\n",
    "#     \"BioSample.Description.Title\",\n",
    "#     \"BioSample.Ids.Id\",\n",
    "#     \"BioSample.Links.Link\",\n",
    "#     \"BioSample.Models.Model\",\n",
    "#     \"BioSample.Owner.Name\",\n",
    "#     \"BioSample.Package\",\n",
    "#     \"BioSample.Status\"\n",
    "# ]\n"
   ],
   "id": "dc53bb5f83c7e401",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "def infer_duckdb_type(series, col_name):\n",
    "    # If the column is \"id\", always return BIGINT\n",
    "    if col_name.lower() == \"id\":\n",
    "        return \"BIGINT\"\n",
    "\n",
    "    # Otherwise use simple inference\n",
    "    if pd.api.types.is_integer_dtype(series):\n",
    "        return \"BIGINT\"\n",
    "    elif pd.api.types.is_float_dtype(series):\n",
    "        return \"DOUBLE\"\n",
    "    elif pd.api.types.is_bool_dtype(series):\n",
    "        return \"BOOLEAN\"\n",
    "    return \"TEXT\"\n",
    "\n",
    "def ensure_columns_exist(conn, table_name, df):\n",
    "    table_info = conn.execute(f\"PRAGMA table_info({table_name})\").fetchall()\n",
    "    existing_columns = {col[1].lower() for col in table_info}\n",
    "\n",
    "    new_columns = [c for c in df.columns if c.lower() not in existing_columns]\n",
    "    if new_columns:\n",
    "        print(f\"{datetime.now().isoformat()}: Adding {len(new_columns)} new column(s) to {table_name}.\")\n",
    "    for col in new_columns:\n",
    "        dtype = infer_duckdb_type(df[col], col)\n",
    "        alter_sql = f'ALTER TABLE {table_name} ADD COLUMN \"{col}\" {dtype}'\n",
    "        conn.execute(alter_sql)\n",
    "\n",
    "def insert_df(conn, table_name, df):\n",
    "    table_info = conn.execute(f\"PRAGMA table_info({table_name})\").fetchall()\n",
    "    existing_columns = [col[1] for col in table_info]\n",
    "\n",
    "    for col in existing_columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "\n",
    "    df = df[existing_columns]\n",
    "\n",
    "    conn.register(\"temp_df\", df)\n",
    "    conn.execute(f\"INSERT INTO {table_name} SELECT * FROM temp_df\")\n",
    "    conn.unregister(\"temp_df\")\n",
    "\n",
    "def extract_biosample_data(collection, conn, path=\"BioSample\", max_docs=None, client=None, batch_size=10000):\n",
    "    \"\"\"\n",
    "    Extract scalar data and build a table in DuckDB.\n",
    "    The 'id' column is always BIGINT.\n",
    "    \"\"\"\n",
    "\n",
    "    def process_data(data, id_value):\n",
    "        if isinstance(data, dict):\n",
    "            scalar_data = {k: v for k, v in data.items() if isinstance(v, (str, int, float, bool))}\n",
    "            # Force id to int if possible\n",
    "            scalar_data[\"id\"] = int(id_value) if id_value is not None else None\n",
    "            return pd.DataFrame([scalar_data])\n",
    "        elif isinstance(data, list) and all(isinstance(item, dict) for item in data):\n",
    "            all_scalar_data = []\n",
    "            for item in data:\n",
    "                scalar_data = {k: v for k, v in item.items() if isinstance(v, (str, int, float, bool))}\n",
    "                scalar_data[\"id\"] = int(id_value) if id_value is not None else None\n",
    "                all_scalar_data.append(scalar_data)\n",
    "            if all_scalar_data:\n",
    "                return pd.DataFrame(all_scalar_data)\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    if path != \"BioSample\":\n",
    "        path_parts = path.split(\".\")[1:]\n",
    "    else:\n",
    "        path_parts = []\n",
    "\n",
    "    if client is None:\n",
    "        raise ValueError(\"Client must be provided to start a session for no_cursor_timeout.\")\n",
    "\n",
    "    table_created = False\n",
    "    processed_docs = 0\n",
    "    table_name = path.split(\".\")[-1].replace(\"-\", \"_\").replace(\".\", \"_\").lower()\n",
    "\n",
    "    batch = []\n",
    "    with client.start_session() as session:\n",
    "        cursor = collection.find({}, no_cursor_timeout=True, session=session)\n",
    "\n",
    "        for doc in cursor:\n",
    "            if max_docs is not None and processed_docs >= max_docs:\n",
    "                break\n",
    "\n",
    "            if not path_parts:  # top-level\n",
    "                scalar_data = {k: v for k, v in doc.items() if isinstance(v, (str, int, float, bool))}\n",
    "                # Force id to int if possible\n",
    "                scalar_data[\"id\"] = int(doc[\"id\"]) if \"id\" in doc else None\n",
    "                df = pd.DataFrame([scalar_data])\n",
    "            else:\n",
    "                current_data = doc\n",
    "                for part in path_parts:\n",
    "                    current_data = current_data.get(part)\n",
    "                    if current_data is None:\n",
    "                        break\n",
    "                if current_data is not None:\n",
    "                    df = process_data(current_data, doc.get('id'))\n",
    "                else:\n",
    "                    df = None\n",
    "\n",
    "            if df is not None and not df.empty:\n",
    "                batch.append(df)\n",
    "\n",
    "            processed_docs += 1\n",
    "\n",
    "            if len(batch) >= batch_size:\n",
    "                combined_df = pd.concat(batch, ignore_index=True)\n",
    "\n",
    "                if not table_created:\n",
    "                    # Use the first batch to create the table\n",
    "                    schema_parts = []\n",
    "                    for col in combined_df.columns:\n",
    "                        dtype = infer_duckdb_type(combined_df[col], col)\n",
    "                        schema_parts.append(f'\"{col}\" {dtype}')\n",
    "                    schema_sql = \", \".join(schema_parts)\n",
    "                    conn.execute(f\"CREATE TABLE {table_name} ({schema_sql})\")\n",
    "                    table_created = True\n",
    "                else:\n",
    "                    ensure_columns_exist(conn, table_name, combined_df)\n",
    "\n",
    "                insert_df(conn, table_name, combined_df)\n",
    "                print(f\"{datetime.now().isoformat()}: Inserted {processed_docs} documents into {table_name} so far.\")\n",
    "                batch.clear()\n",
    "\n",
    "        cursor.close()\n",
    "\n",
    "    if batch:\n",
    "        combined_df = pd.concat(batch, ignore_index=True)\n",
    "        if not table_created:\n",
    "            # Create table if not created yet\n",
    "            schema_parts = []\n",
    "            for col in combined_df.columns:\n",
    "                dtype = infer_duckdb_type(combined_df[col], col)\n",
    "                schema_parts.append(f'\"{col}\" {dtype}')\n",
    "            schema_sql = \", \".join(schema_parts)\n",
    "            conn.execute(f\"CREATE TABLE {table_name} ({schema_sql})\")\n",
    "            table_created = True\n",
    "        else:\n",
    "            ensure_columns_exist(conn, table_name, combined_df)\n",
    "\n",
    "        insert_df(conn, table_name, combined_df)\n",
    "        print(f\"{datetime.now().isoformat()}: Final insert - total {processed_docs} documents processed for {table_name}.\")\n",
    "        batch.clear()\n"
   ],
   "id": "9169b89b35072e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "paths = [\n",
    "    \"BioSample\",\n",
    "    \"BioSample.Attributes.Attribute\",\n",
    "    \"BioSample.Package\",\n",
    "]\n",
    "\n",
    "max_docs =  1000000\n",
    "batch_size = 100000\n",
    "\n",
    "for current_path in paths:\n",
    "    print(f\"Processing path: {current_path}\")\n",
    "    print(datetime.now().isoformat())\n",
    "\n",
    "    extract_biosample_data(collection, duckdb_conn, path=current_path, max_docs=max_docs, client=client, batch_size=batch_size)\n",
    "\n",
    "    print(f\"Completed path: {current_path}\")\n",
    "    print(datetime.now().isoformat())"
   ],
   "id": "2ab978348f80c9ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Close the connection when you're finished\n",
    "duckdb_conn.close()\n",
    "\n",
    "# close the pymongo connection\n",
    "client.close()"
   ],
   "id": "d0ff040778d26d07",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bba68366a8177ca7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
