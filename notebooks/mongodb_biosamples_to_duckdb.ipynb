{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T00:44:59.091245Z",
     "start_time": "2024-12-10T00:44:57.985048Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "from inflection import underscore\n",
    "\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "import pprint\n",
    "\n",
    "import time\n"
   ],
   "id": "e68dcd943609255",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T00:44:59.096775Z",
     "start_time": "2024-12-10T00:44:59.093091Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_duckdb_file(filename):\n",
    "    \"\"\"\n",
    "    Creates a file-based DuckDB database and returns the connection.\n",
    "  \n",
    "    Args:\n",
    "      filename: The name of the DuckDB database file to create.\n",
    "  \n",
    "    Returns:\n",
    "      duckdb.DuckDBPyConnection: The DuckDB connection object.\n",
    "    \"\"\"\n",
    "    conn = duckdb.connect(database=filename)\n",
    "    return conn\n"
   ],
   "id": "e32bd2653ce252d5",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T00:44:59.107083Z",
     "start_time": "2024-12-10T00:44:59.098429Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example usage:\n",
    "filename = \"biosamples.duckdb\"\n",
    "duckdb_conn = create_duckdb_file(filename)"
   ],
   "id": "e9f0e4e3acc0babb",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T00:44:59.133089Z",
     "start_time": "2024-12-10T00:44:59.110069Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# MongoDB connection details\n",
    "connection_string = \"mongodb://localhost:27017/\"\n",
    "db_name = \"biosamples\"\n",
    "collection_name = \"biosamples\"\n",
    "\n",
    "# Connect to MongoDB\n",
    "client = MongoClient(connection_string)\n",
    "db = client[db_name]\n",
    "collection = db[collection_name]"
   ],
   "id": "8bc12c0a82a5b072",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T00:44:59.141525Z",
     "start_time": "2024-12-10T00:44:59.134851Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Retrieve the first document\n",
    "first_document = collection.find_one()\n"
   ],
   "id": "efcac4a393739da5",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T00:44:59.145952Z",
     "start_time": "2024-12-10T00:44:59.143235Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Print the document\n",
    "# pprint.pprint(first_document)"
   ],
   "id": "ae7f62c60dea697b",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T00:44:59.150346Z",
     "start_time": "2024-12-10T00:44:59.147869Z"
    }
   },
   "cell_type": "code",
   "source": "# df['content'].value_counts()",
   "id": "39749d18f30ade87",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T00:44:59.178484Z",
     "start_time": "2024-12-10T00:44:59.152149Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def infer_duckdb_type(series, col_name):\n",
    "    if col_name.lower() == \"id\":\n",
    "        return \"BIGINT\"\n",
    "    if pd.api.types.is_integer_dtype(series):\n",
    "        return \"BIGINT\"\n",
    "    elif pd.api.types.is_float_dtype(series):\n",
    "        return \"DOUBLE\"\n",
    "    elif pd.api.types.is_bool_dtype(series):\n",
    "        return \"BOOLEAN\"\n",
    "    return \"TEXT\"\n",
    "\n",
    "def ensure_columns_exist(conn, table_name, df):\n",
    "    table_info = conn.execute(f\"PRAGMA table_info({table_name})\").fetchall()\n",
    "    existing_columns = {col[1].lower() for col in table_info}\n",
    "    new_columns = [c for c in df.columns if c.lower() not in existing_columns]\n",
    "    if new_columns:\n",
    "        print(f\"{datetime.now().isoformat()}: Adding {len(new_columns)} new column(s) to {table_name}.\")\n",
    "    for col in new_columns:\n",
    "        dtype = infer_duckdb_type(df[col], col)\n",
    "        alter_sql = f'ALTER TABLE {table_name} ADD COLUMN \"{col}\" {dtype}'\n",
    "        conn.execute(alter_sql)\n",
    "\n",
    "def insert_df(conn, table_name, df):\n",
    "    table_info = conn.execute(f\"PRAGMA table_info({table_name})\").fetchall()\n",
    "    existing_columns = [col[1] for col in table_info]\n",
    "\n",
    "    for col in existing_columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "\n",
    "    df = df[existing_columns]\n",
    "\n",
    "    conn.register(\"temp_df\", df)\n",
    "    conn.execute(f\"INSERT INTO {table_name} SELECT * FROM temp_df\")\n",
    "    conn.unregister(\"temp_df\")\n",
    "\n",
    "def process_data(data, id_value):\n",
    "    if isinstance(data, dict):\n",
    "        scalar_data = {k: v for k, v in data.items() if isinstance(v, (str, int, float, bool))}\n",
    "        scalar_data[\"id\"] = int(id_value) if id_value is not None else None\n",
    "        return pd.DataFrame([scalar_data])\n",
    "    elif isinstance(data, list) and all(isinstance(item, dict) for item in data):\n",
    "        all_scalar_data = []\n",
    "        for item in data:\n",
    "            scalar_data = {k: v for k, v in item.items() if isinstance(v, (str, int, float, bool))}\n",
    "            scalar_data[\"id\"] = int(id_value) if id_value is not None else None\n",
    "            all_scalar_data.append(scalar_data)\n",
    "        if all_scalar_data:\n",
    "            return pd.DataFrame(all_scalar_data)\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def extract_all_paths_data(collection, conn, paths, max_docs=None, client=None, batch_size=10000):\n",
    "    \"\"\"\n",
    "    Process multiple paths in a single scan of the MongoDB collection.\n",
    "    Provides verbose status updates:\n",
    "      - A start message at the beginning.\n",
    "      - Roughly every minute, prints how many docs have been processed so far.\n",
    "      - Messages when flushing batches and at the end.\n",
    "    \"\"\"\n",
    "\n",
    "    if client is None:\n",
    "        raise ValueError(\"Client must be provided to start a session for no_cursor_timeout.\")\n",
    "\n",
    "    # Data structures to track per-path info\n",
    "    path_info = {}\n",
    "    for path in paths:\n",
    "        table_name = path.split(\".\")[-1].replace(\"-\", \"_\").replace(\".\", \"_\").lower()\n",
    "        path_info[path] = {\n",
    "            \"table_name\": table_name,\n",
    "            \"table_created\": False,\n",
    "            \"batch\": [],\n",
    "            \"processed_docs\": 0  # how many docs contributed rows for this path\n",
    "        }\n",
    "\n",
    "    def flush_batch(path, combined_df):\n",
    "        info = path_info[path]\n",
    "        if not info[\"table_created\"]:\n",
    "            # Create table from the first batch\n",
    "            schema_parts = []\n",
    "            for col in combined_df.columns:\n",
    "                dtype = infer_duckdb_type(combined_df[col], col)\n",
    "                schema_parts.append(f'\"{col}\" {dtype}')\n",
    "            schema_sql = \", \".join(schema_parts)\n",
    "            conn.execute(f\"CREATE TABLE {info['table_name']} ({schema_sql})\")\n",
    "            info[\"table_created\"] = True\n",
    "        else:\n",
    "            ensure_columns_exist(conn, info['table_name'], combined_df)\n",
    "\n",
    "        insert_df(conn, info['table_name'], combined_df)\n",
    "        print(f\"{datetime.now().isoformat()}: Flushed batch for {info['table_name']}, total {info['processed_docs']} docs processed for this path so far.\")\n",
    "        info[\"batch\"].clear()\n",
    "\n",
    "    # Print a start message\n",
    "    print(f\"{datetime.now().isoformat()}: Starting extraction for paths: {paths}\")\n",
    "    start_time = time.time()\n",
    "    last_status_time = start_time\n",
    "\n",
    "    with client.start_session() as session:\n",
    "        cursor = collection.find({}, no_cursor_timeout=True, session=session)\n",
    "        doc_count = 0\n",
    "        for doc in cursor:\n",
    "            if max_docs is not None and doc_count >= max_docs:\n",
    "                break\n",
    "            doc_count += 1\n",
    "\n",
    "            # Extract data for each path\n",
    "            for path in paths:\n",
    "                if path == \"BioSample\":\n",
    "                    # top-level\n",
    "                    scalar_data = {k: v for k, v in doc.items() if isinstance(v, (str, int, float, bool))}\n",
    "                    scalar_data[\"id\"] = int(doc[\"id\"]) if \"id\" in doc else None\n",
    "                    df = pd.DataFrame([scalar_data]) if scalar_data else None\n",
    "                else:\n",
    "                    path_parts = path.split(\".\")[1:]\n",
    "                    current_data = doc\n",
    "                    for part in path_parts:\n",
    "                        current_data = current_data.get(part)\n",
    "                        if current_data is None:\n",
    "                            break\n",
    "                    if current_data is not None:\n",
    "                        df = process_data(current_data, doc.get('id'))\n",
    "                    else:\n",
    "                        df = None\n",
    "\n",
    "                if df is not None and not df.empty:\n",
    "                    info = path_info[path]\n",
    "                    info[\"batch\"].append(df)\n",
    "                    info[\"processed_docs\"] += 1\n",
    "\n",
    "                    # Check if we need to flush for this path\n",
    "                    if len(info[\"batch\"]) >= batch_size:\n",
    "                        combined_df = pd.concat(info[\"batch\"], ignore_index=True)\n",
    "                        flush_batch(path, combined_df)\n",
    "\n",
    "            # Periodic status update roughly every minute\n",
    "            current_time = time.time()\n",
    "            if (current_time - last_status_time) > 60:\n",
    "                # Print a status message\n",
    "                print(f\"{datetime.now().isoformat()}: Processed {doc_count} documents so far.\")\n",
    "                for p, info in path_info.items():\n",
    "                    print(f\"  Path: {p}, Table: {info['table_name']}, Docs for path: {info['processed_docs']}, Batch size: {len(info['batch'])}\")\n",
    "                last_status_time = current_time\n",
    "\n",
    "        cursor.close()\n",
    "\n",
    "    # Flush remaining batches\n",
    "    for path, info in path_info.items():\n",
    "        if info[\"batch\"]:\n",
    "            combined_df = pd.concat(info[\"batch\"], ignore_index=True)\n",
    "            flush_batch(path, combined_df)\n",
    "            print(f\"{datetime.now().isoformat()}: Final flush - total {info['processed_docs']} documents processed for {info['table_name']}.\")\n",
    "\n",
    "    # Print a completion message\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"{datetime.now().isoformat()}: Completed extraction. Processed {doc_count} documents in {total_time:.2f} seconds.\")\n",
    "    for p, info in path_info.items():\n",
    "        print(f\"  Path: {p}, Table: {info['table_name']}, Total Docs: {info['processed_docs']}\")\n"
   ],
   "id": "9169b89b35072e8",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-12-10T00:44:59.180825Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# paths = [\n",
    "#     \"BioSample\",\n",
    "#     \"BioSample.Attributes.Attribute\",\n",
    "#     \"BioSample.Curation\",\n",
    "#     \"BioSample.Description.Comment.Paragraph\",\n",
    "#     \"BioSample.Description.Organism\",\n",
    "#     \"BioSample.Description.Organism.OrganismName\",\n",
    "#     \"BioSample.Description.Synonym\",\n",
    "#     \"BioSample.Description.Title\",\n",
    "#     \"BioSample.Ids.Id\",\n",
    "#     \"BioSample.Links.Link\",\n",
    "#     \"BioSample.Models.Model\",\n",
    "#     \"BioSample.Owner.Name\",\n",
    "#     \"BioSample.Package\",\n",
    "#     \"BioSample.Status\"\n",
    "# ]\n",
    "\n",
    "paths = [\n",
    "    \"BioSample\",\n",
    "    \"BioSample.Attributes.Attribute\",\n",
    "    \"BioSample.Links.Link\",\n",
    "    \"BioSample.Package\",\n",
    "]\n",
    "\n",
    "max_docs = 45_000_000\n",
    "batch_size =   10_000\n",
    "\n",
    "extract_all_paths_data(collection, duckdb_conn, paths, max_docs=max_docs, client=client, batch_size=batch_size)\n"
   ],
   "id": "389b7a0188a7261b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-09T19:44:59.183175: Starting extraction for paths: ['BioSample', 'BioSample.Attributes.Attribute', 'BioSample.Links.Link', 'BioSample.Package']\n",
      "2024-12-09T19:45:12.702882: Flushed batch for biosample, total 10000 docs processed for this path so far.\n",
      "2024-12-09T19:45:13.253990: Flushed batch for package, total 10000 docs processed for this path so far.\n",
      "2024-12-09T19:45:19.074046: Flushed batch for attribute, total 10000 docs processed for this path so far.\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Close the connection when you're finished\n",
    "duckdb_conn.close()\n",
    "\n",
    "# close the pymongo connection\n",
    "client.close()"
   ],
   "id": "d0ff040778d26d07",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "```2024-12-09T08:43:30.237275: Starting extraction for paths: ['BioSample', 'BioSample.Attributes.Attribute', 'BioSample.Curation', 'BioSample.Description.Comment.Paragraph', 'BioSample.Description.Organism', 'BioSample.Description.Organism.OrganismName', 'BioSample.Description.Synonym', 'BioSample.Description.Title', 'BioSample.Ids.Id', 'BioSample.Links.Link', 'BioSample.Models.Model', 'BioSample.Owner.Name', 'BioSample.Package', 'BioSample.Status']```\n",
    "\n",
    "16,666.7 docs/min including all paths above\n",
    "120,385 docs/minute for BioSample.Attributes.Attribute alone\n",
    "\n",
    "started at 08:43\n",
    "\n",
    "1 Million docs process will likely complete around 9:43.\n",
    "\n",
    "not processing BioSample.Owner.Contacts (highly nested). we are processing BioSample.Owner.Name\n",
    "\n",
    "not processing BioSample.Description.Comment.Paragraph.Text (highly nested). we are processing BioSample.Description.Comment.Paragraph\n"
   ],
   "id": "ca3b5309e8b19328"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "3531c9f30927a0ac",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
