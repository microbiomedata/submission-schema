{
 "cells": [
  {
   "cell_type": "code",
   "id": "630126a7790cc3a7",
   "metadata": {},
   "source": [
    "from common import *\n",
    "\n",
    "import gzip\n",
    "import os\n",
    "import shutil\n",
    "import sqlite3\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import requests\n",
    "from oaklib import get_adapter\n",
    "from oaklib.datamodels.vocabulary import IS_A\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import yaml\n",
    "import json\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7b0ae730412c28ca",
   "metadata": {},
   "source": [
    "print(\"verify output is being rendered\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f952b5888d2c5f0d",
   "metadata": {},
   "source": [
    "# Initialize cache dictionaries for predict_from_normalized_env_packages\n",
    "# todo how to move the definitions for function that use these globals? Or just use caching around the function?\n",
    "ancestor_cache = {}\n",
    "descendant_cache = {}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5001cbd978bbdd28",
   "metadata": {},
   "source": [
    "# todo deal with circularity in env package prediction -> env triad reporting\n",
    "\n",
    "# todo this on-demand NCBI curie extraction and annotation recapitulates work that is being added to\n",
    "# https://portal.nersc.gov/project/m3408/biosamples_duckdb/\n",
    "# via \n",
    "#   although that doesn't detect auto-incremented curies from  spreadsheet dragging\n",
    "\n",
    "# todo if more caching is desired, it should probably take the form of saving dataframes for TSV\n",
    "\n",
    "# eventually, dig up a complete JSON gold biosample dump for non-hybrid biosample counts\n",
    "\n",
    "# overall run time (if NCBI biosamples and goldData are cached): ~ 10 minutes\n",
    "\n",
    "# count studies not biosamples ?\n",
    "# how? for gold, ncbi or nmdc?"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2ef284816f7ce9d6",
   "metadata": {},
   "source": [
    "# Task Settings\n",
    "_For making a Soil env_broad_scale voting sheet vs a Sediment env_local_scale sheet, etc._\n",
    "\n",
    "todo: bundle these into dicts so they don't have to be modified independently and kept in sync with one another."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "output_file_name = \"voting_sheets_output/soil_env_local_scale_voting_sheet.tsv\"",
   "id": "90bf7256a956701d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# semantic_anchor = 'ENVO:00000428' # biome for env_broad_scale\n",
    "semantic_anchor = 'ENVO:01000813' # astronomical body part \"abp\" for env_local_scale\n",
    "# semantic_anchor = 'ENVO:00010483' # environmental material for env_medium"
   ],
   "id": "95f7efc06f20987a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## context selectors",
   "id": "51f51b3b88634488"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "gold_context_selectors =  [\n",
    "    'mixs:env_broad',\n",
    "    'mixs:env_local',\n",
    "    'mixs:env_medium'\n",
    "]\n"
   ],
   "id": "be00ed63d6472ead",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ncbi_context_selector = 'env_broad_scale'\n",
    "ncbi_context_selector = 'env_local_scale'\n",
    "# ncbi_context_selector = 'env_medium'"
   ],
   "id": "7aa7b42c3f20ce44",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# nmdc_context_selector= 'env_broad_scale_id'\n",
    "nmdc_context_selector= 'env_local_scale_id'\n",
    "# nmdc_context_selector= 'env_medium_id'"
   ],
   "id": "ce80b2d7e64e5710",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## package aka environment aka extension selectors",
   "id": "2f0bdadeb5d32fa8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# plant_first_where = \"s1.value like 'host-associated > plants%'\"\n",
    "# sediment_first_where = \"lower(s1.value) like 'environmental > aquatic%sediment%'\"\n",
    "# soil_first_where = \"s1.value like 'environmental > terrestrial > soil%'\"\n",
    "# water_first_where = \"s1.value like 'environmental > aquatic%' and lower(s1.value) not like '%sediment%'\"\n",
    "\n",
    "plant_first_where = \"lower(s1.value) like '%plant%'\" # picks up waste water treatment plant\n",
    "sediment_first_where = \"lower(s1.value) like '%sediment%'\"\n",
    "soil_first_where = \"lower(s1.value) like '%soil%'\"\n",
    "water_first_where = \"lower(s1.value) like '%aquatic%' and lower(s1.value) not like '%sediment%'\""
   ],
   "id": "def7edf88043ac30",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "gold_first_where = soil_first_where",
   "id": "8075741d803ff740",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# todo new since soil: why are we only considering MIMS.me for discovering appropriate env triad values?\n",
    "#   there's usually a roughly equal number of biosamples from in each extension for MIMS.me and \n",
    "\n",
    "# ncbi_package_selector = 'plant-associated.6.0'\n",
    "# ncbi_package_selector = 'sediment.6.0'\n",
    "ncbi_package_selector = 'soil.6.0'\n",
    "# ncbi_package_selector = 'water.6.0'"
   ],
   "id": "f5654d6f9319576b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# nmdc_package_selector = 'plant-associated'\n",
    "# nmdc_package_selector = 'sediment'\n",
    "nmdc_package_selector = 'soil'\n",
    "# nmdc_package_selector = 'water'\n"
   ],
   "id": "5e358e42ad885188",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "GOLDTERMS_NA = '' # ???\n",
    "\n",
    "GOLDTERMS_PLANT_ASSOCIATED = GOLDTERMS_NA # host associated -> viridiplantae? take a string approach!\n",
    "GOLDTERMS_SEDIMENT = 'GOLDTERMS:3985' #  doesn't have any subclasses\n",
    "GOLDTERMS_SOIL = 'GOLDTERMS:4212'\n",
    "GOLDTERMS_WATER = 'GOLDTERMS:3984'\n",
    "\n",
    "# GOLDTERMS:4180, 'Environmental > Aquatic > Freshwater > Pond > Sediment' and ~64 more don't share a common root\n",
    "# poetry run runoak -i sqlite:obo:goldterms info 't~sediment'\n"
   ],
   "id": "eda591e14a91cd01",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "goldterms_root = GOLDTERMS_SOIL",
   "id": "bba8bf1e75430ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## selecting name and version of one enum for comparison\n",
   "id": "394f96a70a95754f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# only the Soil enums have legacy definitions (v10.7 and earlier?)\n",
    "\n",
    "# CONTEXT_ENUM = \"EnvBroadScaleSoilEnum\"\n",
    "CONTEXT_ENUM = \"EnvLocalScaleSoilEnum\"\n",
    "# CONTEXT_ENUM = \"EnvMediumSoilEnum\"\n",
    "\n",
    "# CONTEXT_ENUM = \"\""
   ],
   "id": "20750df32ebb2033",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# todo: add columns for membership in multiple enums from multiple version of the schema?\n",
    "#  like sediment local vs soil local and water local (once that's completed)\n",
    "#  get them from schema files or something prior to that? sems like the voting sheets are too raw/preliminary for that\n",
    "#   can use a more recent schema url for more recent enums!\n",
    "\n",
    "previous_submission_schema_url = \"https://raw.githubusercontent.com/microbiomedata/submission-schema/v10.7.0/src/nmdc_submission_schema/schema/nmdc_submission_schema.yaml\"\n",
    "\n",
    "# previous_submission_schema_url = \"https://raw.githubusercontent.com/microbiomedata/submission-schema/refs/tags/v11.1.0/src/nmdc_submission_schema/schema/nmdc_submission_schema.yaml\""
   ],
   "id": "f588eccfcb2771ba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# todo: don't call the column \"legacy_pv\". use the name of the enum and the version of the schema?\n",
    "\n",
    "comparison_enum_column_name = 'EnvLocalSoilEnum_10_7'\n",
    "# comparison_enum_column_name = 'EnvLocalScaleSoilEnum_11_1'\n",
    "# comparison_enum_column_name = 'no_comparison_enum'"
   ],
   "id": "e259066dee0973c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Additional Settings",
   "id": "b799aabdd66098c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Approved prefixes (case-insensitive)\n",
    "approved_prefixes = ['ENVO']"
   ],
   "id": "50e39f658f4bd77b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "MIN_ANNOTATION_LEN = 3",
   "id": "3aa0b52456f0a95c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d526be618b8b9129",
   "metadata": {},
   "source": [
    "NMDC_RUNTIME_BASE_URL = 'https://api.microbiomedata.org/nmdcschema/'\n",
    "STUDY_SET_COLLECTION = 'study_set'\n",
    "BIOSAMPLE_SET_COLLECTION = 'biosample_set'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "11c0cba800e33ac6",
   "metadata": {},
   "source": [
    "envo_adapter_string = \"sqlite:obo:envo\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7795121f47307a00",
   "metadata": {},
   "source": [
    "env_package_override_file = 'mam-env-package-overrides.tsv'\n",
    "override_column = 'mam_inferred_env_package'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f1b99451d88ce356",
   "metadata": {},
   "source": [
    "# ncbi_duckdb_url = 'https://portal.nersc.gov/project/m3408/biosamples_duckdb/ncbi_biosamples_2024-09-23.duckdb.gz'\n",
    "ncbi_duckdb_url = 'https://portal.nersc.gov/project/m3408/biosamples_duckdb/ncbi_biosamples.duckdb.gz'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "51e5d41b02b19eec",
   "metadata": {},
   "source": [
    "gold_data_url = \"https://gold.jgi.doe.gov/download?mode=site_excel\"\n",
    "gold_data_file_name = \"goldData.xlsx\" # goldData.xlsx: Microsoft Excel 2007+\n",
    "gold_csv_file_name = \"gold_biosamples.csv\"\n",
    "BIOSAMPLES_SHEET = \"Biosample\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e1d9f705fad2efa9",
   "metadata": {},
   "source": [
    "goldterms_semsql_url = \"https://s3.amazonaws.com/bbop-sqlite/goldterms.db.gz\"\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "all_nmdc_biosamples_file = 'all_nmdc_biosamples.json'",
   "id": "db6170cb9f3a6ca0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# CURIe Constants",
   "id": "16e21c18886a373c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "BIOME = 'ENVO:00000428'\n",
    "TERRESTRIAL_BIOME = 'ENVO:00000446'\n",
    "AQUATIC_BIOME = 'ENVO:00002030'\n",
    "ABP = 'ENVO:01000813'\n",
    "ENVIRONMENTAL_SYSTEM = 'ENVO:01000254'\n",
    "ENVIRONMENTAL_MATERIAL = 'ENVO:00010483'\n",
    "\n",
    "SOIL = 'ENVO:00001998'\n",
    "LIQUID_WATER = 'ENVO:00002006'\n",
    "WATER_ICE = 'ENVO:01000277'\n",
    "\n",
    "HUMAN_CONSTRUCTION = 'ENVO:00000070'\n",
    "BUILDING = 'ENVO:00000073'\n",
    "BUILDING_PART = 'ENVO:01000420'"
   ],
   "id": "73e52103eb69bbcc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Settings-based Queries",
   "id": "7ecec14181910236"
  },
  {
   "cell_type": "code",
   "id": "4cdc7819c92bff3c",
   "metadata": {},
   "source": [
    "goldterms_subclass_query = f\"\"\"\n",
    "select\n",
    "\tsubject\n",
    "from\n",
    "\tentailed_edge ee\n",
    "where\n",
    "\tpredicate = 'rdfs:subClassOf'\n",
    "\tand object = '{goldterms_root}'\n",
    "\"\"\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fb8c7caf28e64d10",
   "metadata": {},
   "source": [
    "# todo could this have been done with a OAK query, eliminating the need to explicitly download the file?\n",
    "\n",
    "goldterms_envo_query = f\"\"\"\n",
    "SELECT\n",
    "\t*\n",
    "FROM\n",
    "\tstatements s\n",
    "WHERE\n",
    "\tpredicate in ('{\"', '\".join(gold_context_selectors)}')\"\"\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##  Current task: do the counting by Bioproject instead of or in addition to this",
   "id": "82a129867b2d9aa3"
  },
  {
   "cell_type": "code",
   "id": "afd52cf83301de09",
   "metadata": {},
   "source": [
    "ncbi_biosamples_per_annotation_query = f\"\"\"\n",
    "SELECT content, COUNT(1) AS sample_count \n",
    "FROM attributes \n",
    "WHERE harmonized_name = '{ncbi_context_selector}' AND package_content like '%{ncbi_package_selector}'\n",
    "GROUP BY content\n",
    "ORDER BY COUNT(1) DESC\n",
    "\"\"\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# and s1.subject = s1.stanza eliminates matches on blank node annotation rows (probably wouldn't change results but adds a little overhead)\n",
    "\n",
    "extension_query = f\"\"\"\n",
    "select\n",
    "\t\ts1.subject ,\n",
    "\t\ts2.predicate,\n",
    "\t\tCOALESCE (s2.\"object\",\n",
    "\ts2.\"value\") as content\n",
    "from\n",
    "\tstatements s1\n",
    "join statements s2 on \n",
    "\ts1.subject = s2.subject\n",
    "where\n",
    "\t{gold_first_where}\n",
    "\tand s1.predicate = 'rdfs:label'\n",
    "\tand s1.subject = s1.stanza\n",
    "\tand s2.predicate in ('mixs:env_broad', 'mixs:env_local', 'mixs:env_medium', 'mixs:mixs_extension', 'rdfs:label', 'mixs:other', 'mixs:anatomical_site', 'mixs:host_taxon') ;\n",
    "\"\"\"\n"
   ],
   "id": "8cdd3e6e17a2bac3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Locally Defined Functions\n",
    "_Currently using locally-defined cache dictionaries_"
   ],
   "id": "6c4cf0d065a7de07"
  },
  {
   "cell_type": "code",
   "id": "38412bd611726160",
   "metadata": {},
   "source": [
    "def predict_from_normalized_env_packages(df_raw, adapter):\n",
    "    # Apply the function to the relevant columns\n",
    "\n",
    "    df = df_raw.copy()\n",
    "\n",
    "    print(df.shape)\n",
    "    for column in ['env_broad_scale_id', 'env_local_scale_id', 'env_medium_id']:\n",
    "        df[f'{column}_ancestors'] = df[column].apply(lambda x: get_hierarchy_terms(x, adapter)['ancestors'])\n",
    "        df[f'{column}_descendants'] = df[column].apply(lambda x: get_hierarchy_terms(x, adapter)['descendants'])\n",
    "\n",
    "    # Vectorize each set of terms separately\n",
    "    broad_scale_ancestors = vectorize_terms(df, 'env_broad_scale_id_ancestors')\n",
    "    broad_scale_descendants = vectorize_terms(df, 'env_broad_scale_id_descendants')\n",
    "\n",
    "    local_scale_ancestors = vectorize_terms(df, 'env_local_scale_id_ancestors')\n",
    "    local_scale_descendants = vectorize_terms(df, 'env_local_scale_id_descendants')\n",
    "\n",
    "    medium_ancestors = vectorize_terms(df, 'env_medium_id_ancestors')\n",
    "    medium_descendants = vectorize_terms(df, 'env_medium_id_descendants')\n",
    "\n",
    "    # Combine all feature matrices\n",
    "    X = hstack([\n",
    "        broad_scale_ancestors,\n",
    "        broad_scale_descendants,\n",
    "        local_scale_ancestors,\n",
    "        local_scale_descendants,\n",
    "        medium_ancestors,\n",
    "        medium_descendants\n",
    "    ])\n",
    "\n",
    "    # Filter the DataFrame to only include non-null rows for the target column\n",
    "    df_filtered = df[df['normalized_env_package'].notnull() & (df['normalized_env_package'] != \"\")]\n",
    "\n",
    "    # Extract the target variable\n",
    "    y = df_filtered['normalized_env_package']\n",
    "\n",
    "    # Ensure X corresponds to the filtered rows\n",
    "    X_filtered = X[df_filtered.index]\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_filtered, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Train a Random Forest Classifier\n",
    "    clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # not determining confidence for each class nor saving any diagnostics any more\n",
    "\n",
    "    return clf.predict(X)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4515456aaeaa7980",
   "metadata": {},
   "source": [
    "def get_hierarchy_terms(my_curie: str, adapter) -> dict:\n",
    "    \"\"\"\n",
    "    Extract ancestor and descendant terms from the ontology for a given CURIE,\n",
    "    using caching to improve performance and filtering by 'is_a' relationships.\n",
    "\n",
    "    Args:\n",
    "        my_curie (str): CURIE identifier for the ontology term.\n",
    "        adapter: Ontology adapter.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing lists of ancestor and descendant terms.\n",
    "    \"\"\"\n",
    "    if my_curie not in ancestor_cache:\n",
    "        try:\n",
    "            ancestors = list(adapter.ancestors(my_curie, predicates=[IS_A]))\n",
    "            ancestor_cache[my_curie] = [adapter.label(ancestor) for ancestor in ancestors if ancestor]\n",
    "        except Exception as my_e:\n",
    "            print(f\"Error retrieving ancestors for {my_curie}: {my_e}\")\n",
    "            ancestor_cache[my_curie] = []\n",
    "\n",
    "    if my_curie not in descendant_cache:\n",
    "        try:\n",
    "            descendants = list(adapter.descendants(my_curie, predicates=[IS_A]))\n",
    "            descendant_cache[my_curie] = [adapter.label(descendant) for descendant in descendants if descendant]\n",
    "        except Exception as my_e:\n",
    "            print(f\"Error retrieving descendants for {my_curie}: {my_e}\")\n",
    "            descendant_cache[my_curie] = []\n",
    "\n",
    "    return {\n",
    "        'ancestors': ancestor_cache[my_curie],\n",
    "        'descendants': descendant_cache[my_curie],\n",
    "    }"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Procedural Code Starts Here",
   "id": "1423fbbafdf5e167"
  },
  {
   "cell_type": "code",
   "id": "44fb14c1b01ac34c",
   "metadata": {},
   "source": [
    "# Determine the filenames and target directory for the NCBI DuckDB\n",
    "ncbi_compressed_filename = urlparse(ncbi_duckdb_url).path.split('/')[-1]\n",
    "ncbi_filename = os.path.splitext(ncbi_compressed_filename)[0]\n",
    "ncbi_compressed_file_path = os.path.join(ncbi_compressed_filename)\n",
    "ncbi_uncompressed_file_path = os.path.join(ncbi_filename)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "353ba83ec3059ac5",
   "metadata": {},
   "source": [
    "if os.path.isfile(ncbi_uncompressed_file_path):\n",
    "    print(f\"{ncbi_uncompressed_file_path} is already present in the current working directory.\")\n",
    "else:\n",
    "    if os.path.isfile(ncbi_compressed_file_path):\n",
    "        print(f\"{ncbi_compressed_file_path} is already present in the current working directory.\")\n",
    "    else:\n",
    "        print(f\"{ncbi_compressed_file_path} needs to be downloaded\")\n",
    "        ncbi_response = requests.get(ncbi_duckdb_url)\n",
    "        with open(ncbi_compressed_file_path, \"wb\") as f:\n",
    "            f.write(ncbi_response.content)\n",
    "        # ~ 2 minutes @ 250 Mbps\n",
    "    \n",
    "    # Unzip the compressed file and save the extracted file in target directory\n",
    "    print(f\"{ncbi_compressed_file_path} needs to be unpacked\")\n",
    "    with gzip.open(ncbi_compressed_file_path, \"rb\") as f_in:\n",
    "        with open(ncbi_uncompressed_file_path, \"wb\") as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "    # ~ 2 minutes"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "63f815cbfb9b7a6b",
   "metadata": {},
   "source": [
    "ncbi_conn = duckdb.connect(database=ncbi_uncompressed_file_path, read_only=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "198256f26bc060e3",
   "metadata": {},
   "source": [
    "envo_adapter = get_adapter(envo_adapter_string)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Anchor aka bootstrapping classes",
   "id": "491e3ded2d1fba45"
  },
  {
   "cell_type": "code",
   "id": "e863dc687ce4be5c",
   "metadata": {},
   "source": "anchor_descendants = get_curie_descendants_label_dict(semantic_anchor, [IS_A], envo_adapter)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "394abb56c4987eee",
   "metadata": {},
   "source": [
    "anchor_descendants_lod = curie_descendants_label_dict_to_lod(anchor_descendants)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "eea1a3f8395da0d7",
   "metadata": {},
   "source": [
    "anchor_descendants_frame = curie_descendants_label_lod_to_df(anchor_descendants_lod)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f85faf81e0a55a42",
   "metadata": {},
   "source": [
    "anchor_descendants_frame"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "857f1f1efd4635b9",
   "metadata": {},
   "source": "# Classes from the reference enumeration"
  },
  {
   "cell_type": "code",
   "id": "1432828ce7983148",
   "metadata": {},
   "source": [
    "sv = get_schemaview_from_source(previous_submission_schema_url)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5841e0d6bc3d3c40",
   "metadata": {},
   "source": [
    "# todo break out slow steps into its own cell\n",
    "\n",
    "try:\n",
    "    CONTEXT_ENUM_def = sv.get_enum(CONTEXT_ENUM)\n",
    "    context_pvs_keys = list(CONTEXT_ENUM_def.permissible_values.keys())\n",
    "except AttributeError as e:\n",
    "    # Handle the AttributeError\n",
    "    print(f\"An AttributeError occurred: {e}\")\n",
    "    context_pvs_keys =[]\n",
    "    "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(context_pvs_keys)",
   "id": "e47067e5b704ee70",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "79282f64358e1608",
   "metadata": {},
   "source": [
    "initially_parsed_context_pvs = parse_hierarchically_underscored_strings(context_pvs_keys)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5a9492fa605534f2",
   "metadata": {},
   "source": [
    "deduped_context_pvs = dedupe_underscoreless_pvs(initially_parsed_context_pvs)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "214c2e6c9a7089a8",
   "metadata": {},
   "source": [
    "pv_validation_results = validate_curie_label_list_dict(deduped_context_pvs, envo_adapter, print_flag=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e84872aa4f8a9313",
   "metadata": {},
   "source": [
    "pv_validation_results"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "363063c37a433d70",
   "metadata": {},
   "source": "# Get the CURIEs used in NMDC Biosample annotations"
  },
  {
   "cell_type": "code",
   "id": "656e23e9eab154b0",
   "metadata": {},
   "source": [
    "if os.path.isfile(all_nmdc_biosamples_file):\n",
    "    print(f\"{all_nmdc_biosamples_file} is present in the current working directory and will be read into all_nmdc_biosamples.\")\n",
    "    # with open(all_nmdc_biosamples_file, 'r') as file:\n",
    "    #     all_nmdc_biosamples = yaml.full_load(file)\n",
    "    # read as json\n",
    "    with open(all_nmdc_biosamples_file, 'r') as f:\n",
    "        all_nmdc_biosamples = json.load(f)\n",
    "\n",
    "else:\n",
    "    print(f\"All NMDC Biosamples need to be fetched and saved to {all_nmdc_biosamples_file}\")\n",
    "    all_nmdc_biosamples = get_docs_from_nmdc_collection(NMDC_RUNTIME_BASE_URL,\n",
    "                                               BIOSAMPLE_SET_COLLECTION)\n",
    "    # with open(all_nmdc_biosamples_file, 'w') as file:\n",
    "    #     documents = yaml.dump(all_nmdc_biosamples, file)\n",
    "    # save as json\n",
    "    with open(all_nmdc_biosamples_file, 'w') as f:\n",
    "        json.dump(all_nmdc_biosamples, f)\n",
    "\n",
    "# this saves network traffic. could use JSON for faster performance. \n",
    "# 1 minute for network fetch and JSON write?!\n",
    "# 1 minute for yaml read\n",
    "# instantaneous for JSON read?"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Inference of env_package annotations ",
   "id": "1566ce7dbafcfc71"
  },
  {
   "cell_type": "code",
   "id": "6a17544640272855",
   "metadata": {},
   "source": [
    "env_pacakge_overrides = tsv_to_dict_of_dicts(env_package_override_file, 'id')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "66c61cf229a6545c",
   "metadata": {},
   "source": [
    "# todo show env_pacakge_overrides as a data frame\n",
    "#   with some other columns for context?"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7045884ea68d0351",
   "metadata": {},
   "source": [
    "biosample_contexts_lod = biosamples_lod_context_extractor(all_nmdc_biosamples, envo_adapter,\n",
    "                                                          my_env_pacakge_overrides=env_pacakge_overrides)\n",
    "\n",
    "# ~ 10 seconds, lots of logging"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7cf668c88434c41e",
   "metadata": {},
   "source": [
    "nmdc_biosample_contexts_frame = pd.DataFrame(biosample_contexts_lod)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "77e340a7962f23a6",
   "metadata": {},
   "source": [
    "# print a value count for the normalized_env_package column\n",
    "print(\"Value counts for normalized_env_package column:\")\n",
    "print(nmdc_biosample_contexts_frame['normalized_env_package'].value_counts(dropna=False))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5980aef19fb86ae4",
   "metadata": {},
   "source": [
    "package_predictions = predict_from_normalized_env_packages(nmdc_biosample_contexts_frame, envo_adapter)\n",
    "\n",
    "# these predictions often have a f1 of 1.00\n",
    "# many people might find that hard to believe"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e151c95fe349df93",
   "metadata": {},
   "source": [
    "nmdc_biosample_contexts_frame['predicted_env_package'] = package_predictions"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## env-package inference complete\n",
    "\n",
    "To-do: save this and don't recreate it if it's available\n",
    "\n",
    "Then get it reviewed by other NMDC stakeholders and inject it into MongoDB if approved"
   ],
   "id": "a86897dcc7d47c55"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Destructively filter `nmdc_biosample_contexts_frame` by `env_package` ",
   "id": "a3f22287b11f918f"
  },
  {
   "cell_type": "code",
   "id": "d5c6aa902c7187e0",
   "metadata": {},
   "source": [
    "nmdc_biosample_contexts_frame.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2b241bcacf4bb100",
   "metadata": {},
   "source": [
    "nmdc_biosample_contexts_frame = nmdc_biosample_contexts_frame[\n",
    "    nmdc_biosample_contexts_frame['predicted_env_package'] == nmdc_package_selector]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9045896aa4ea468f",
   "metadata": {},
   "source": [
    "nmdc_biosample_contexts_frame.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "aee7b4eea48a8691",
   "metadata": {},
   "source": "# Long process of inferring OBO foundry CURIes from NCBI Biosamples"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Start by getting unique annotations? Pre-counted by Biosamples\n",
    "\n",
    "Current task is to provide counts by \"study\" aka Bioproject in addition to Biosample counts or instead of Biosamples counts if necessary"
   ],
   "id": "fc58f6a0b79cbfa1"
  },
  {
   "cell_type": "code",
   "id": "3be362b73dbda47a",
   "metadata": {},
   "source": "ncbi_frame = ncbi_conn.execute(ncbi_biosamples_per_annotation_query).fetchdf()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "306ee79ce77f745e",
   "metadata": {},
   "source": [
    "ncbi_frame.insert(0, 'serial_number', range(1, len(ncbi_frame) + 1))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6f06a6237f0ecdb",
   "metadata": {},
   "source": [
    "# includes values with counts of one... useful for discovering drag-down submissions?"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## MIxS and NCBI guidelines imply environmental context slots are multivalued\n",
    "and that the pipe `|` should be used as a delimiter\n",
    "\n",
    "there's an envo_count value below that indicates how ofter other delimiteres might be used"
   ],
   "id": "1b851c8ef274382c"
  },
  {
   "cell_type": "code",
   "id": "f07e55ebd0f4c451",
   "metadata": {},
   "source": [
    "ncbi_frame['content_list'] = ncbi_frame['content'].str.split('|')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# todo is there any reason to not do this ?\n",
    "ncbi_frame = ncbi_frame[ncbi_frame['content'].notna() & (ncbi_frame['content'] != '')]"
   ],
   "id": "2e96404aa5dd0f7b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f8c2e13cee8818cb",
   "metadata": {},
   "source": [
    "ncbi_frame['content_count'] = ncbi_frame['content_list'].apply(len)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "edeaac7b5ce53cd5",
   "metadata": {},
   "source": [
    "ncbi_frame.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "18860251fea32ee5",
   "metadata": {},
   "source": [
    "ncbi_frame = ncbi_frame.explode('content_list').reset_index(drop=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bc8b89960c12f6e6",
   "metadata": {},
   "source": [
    "ncbi_frame.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## splitting adds ~ 5% more rows\n",
    "which might be important since were currently using a longest annotation strategy here"
   ],
   "id": "f02aaf625f4158d3"
  },
  {
   "cell_type": "code",
   "id": "85a0a4ccd63b7952",
   "metadata": {},
   "source": [
    "# how many content_list strings contain envo multiple times now?"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4dee374026c8272a",
   "metadata": {},
   "source": [
    "ncbi_frame['envo_count'] = ncbi_frame['content_list'].str.lower().str.count(\"envo\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "417319d62ef07d0b",
   "metadata": {},
   "source": [
    "ncbi_frame['envo_count'].value_counts()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## If my math is correct, about 0.1% of the annotations still contain multiple CURIes \n",
    "after splitting on pipes\n",
    "\n",
    "There will also be annotations with multiple label-like strings that weren't split because they weren't delimited on pipes\n",
    "That might be a source of lost information since we are using a longest-match annotator here\n",
    "I.e. there could be annotations with multiple hits worth keeping"
   ],
   "id": "21fb4262b735aedc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Parsing out CURIEs\n",
    "\n",
    "this has a few limitations. The function only tries pre-specified prefixes (['ENOV'] by default) and only considers colons and underscores valid delimiters."
   ],
   "id": "119dbec6ecf9c65c"
  },
  {
   "cell_type": "code",
   "id": "7b256380eceaceaa",
   "metadata": {},
   "source": [
    "ncbi_frame[['extracted_label', 'extracted_curie']] = ncbi_frame['content_list'].apply(parse_curie_label)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "761e39ab29aef0a7",
   "metadata": {},
   "source": [
    "parse_failures = ncbi_frame[\n",
    "    (ncbi_frame['envo_count'] > 0) & (ncbi_frame['extracted_curie'].isna() | (ncbi_frame['extracted_curie'] == ''))]\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## In what kinds of cases could no CURIe be parsed\n",
    "despite the presence of \"ENVO\" in the content string?"
   ],
   "id": "e618518e6e85b5aa"
  },
  {
   "cell_type": "code",
   "id": "a9316f11da9ee0c8",
   "metadata": {},
   "source": [
    "parse_failures"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Retrieve the labels for the parsed CURIes",
   "id": "ab4b8ef4ac9758e2"
  },
  {
   "cell_type": "code",
   "id": "9bdebd91856ff679",
   "metadata": {},
   "source": [
    "ncbi_frame['real_label'] = ncbi_frame['extracted_curie'].apply(envo_adapter.label)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Apply oaklib annotation to the strings after CURIe removal\n",
    "Actually the annotator can (sometimes?) detect colon-delimited CURIEs with lower case prefixes\n",
    "\n",
    "This returns CURIes with evidence but not necessarily the label corresponding to the CURIe"
   ],
   "id": "9710b8f5b9a39057"
  },
  {
   "cell_type": "code",
   "id": "d06d899567db022e",
   "metadata": {},
   "source": [
    "# Apply the annotation function to each row in the 'label' column\n",
    "ncbi_frame['longest_annotation_curie'] = ncbi_frame['extracted_label'].apply(\n",
    "    lambda x: get_longest_annotation_curie(x, envo_adapter, MIN_ANNOTATION_LEN))\n",
    "\n",
    "# this cell only takes ~ 1 minute, but generates a lot of \"ERRORS\" and WARNINGS in a red fornt\n",
    "#   while loading the ontologies that are used for annotating\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Add the labels for the CURIes identified though oaklib annotation of strings",
   "id": "14f0908428dfa322"
  },
  {
   "cell_type": "code",
   "id": "2d0d090813223db3",
   "metadata": {},
   "source": [
    "ncbi_frame['longest_annotation_label'] = ncbi_frame['longest_annotation_curie'].apply(envo_adapter.label)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "940cd4e688adb239",
   "metadata": {},
   "source": [
    "ncbi_frame"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## we now have a list of CURIes for each normalized annotation\n",
    "\n",
    "This could be because the submitter provided a CURIe and a label that don't match\n",
    "*One* case of this is dragging a CURIe down a column in a spreadsheet, expecting it to be copied,\n",
    "but actually auto-incrementing it\n",
    "\n",
    "Now attempt to find one best CURIe for each annotation... by now we have lost the ability to retain multiple legitimate\n",
    "but improperly separated CURIes"
   ],
   "id": "69ed214e0186f459"
  },
  {
   "cell_type": "code",
   "id": "951ca41984b81e7d",
   "metadata": {},
   "source": [
    "# todo don't accept extracted curie if no real label?\n",
    "# any kind of string similarity checking for label of annotated curie vs extracted label ?\n",
    "# look for long stretches of curies?\n",
    "# can we measure the beneficial impact of any of this? current crux: how to distribute counts\n",
    "\n",
    "ncbi_frame['curie_list'] = ncbi_frame.apply(\n",
    "    lambda my_row: list({my_row['extracted_curie'], my_row['longest_annotation_curie']} - {None}),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "ncbi_frame['unique_curie_count'] = ncbi_frame['curie_list'].apply(len)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "349610c477ebf149",
   "metadata": {},
   "source": [
    "ncbi_frame['unique_curie_count'].value_counts()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e6a8c0273f36e906",
   "metadata": {},
   "source": [
    "double_curie_frame = ncbi_frame[ncbi_frame['unique_curie_count'] > 1]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "334e7c646337a914",
   "metadata": {},
   "source": [
    "double_curie_frame = double_curie_frame[['extracted_curie', 'longest_annotation_curie']]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "874149830df3dd8a",
   "metadata": {},
   "source": [
    "double_curie_frame = double_curie_frame.drop_duplicates()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cdfc0bda39f3ad6d",
   "metadata": {},
   "source": [
    "double_curie_frame[['extracted_prefix', 'extracted_local_id']] = double_curie_frame['extracted_curie'].str.split(':', expand=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "de6fcee63f6677d",
   "metadata": {},
   "source": [
    "double_curie_frame['extracted_local_id_int'] = pd.to_numeric(double_curie_frame['extracted_local_id'], errors='coerce').astype('Int64')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "double_curie_frame",
   "id": "185c2d7e43614639",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fae831b04ed985ef",
   "metadata": {},
   "source": [
    "# Ensure extracted_local_id_int is unique and sorted\n",
    "unique_sorted_series = double_curie_frame['extracted_local_id_int'].dropna().drop_duplicates().sort_values()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d244d7b150a3b413",
   "metadata": {},
   "source": [
    "# Find stretches\n",
    "stretches_dict = find_consecutive_stretches_dict(unique_sorted_series)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2fda8122842f63c0",
   "metadata": {},
   "source": [
    "# Convert the stretches dictionary into a DataFrame\n",
    "stretches_df = stretches_dict_to_long_dataframe(stretches_dict)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`stretches_df` shows groups of extracted EnvoO ids (CURIes without prefix or padding zeros) that share a common CURIe by oaklib annotation of the textual part. This may not be the best or only way to address these spurious drag-stretch, auto-incremented CURIes\n",
    "\n",
    "Ie 1001458 corresponds to ENVO:01001458, 'mist'\n",
    "\n",
    "_although it theoretically could have been ENVO:1001458 since EnvO CURIes can have 7 or 8 digits_\n",
    "\n",
    "In group 9, there are another ~ 50 sequential id values, all corresponding to environmental context annotations whose best oak-annotated class is ENVO:01001803, 'tropical forest'!\n",
    "\n",
    "How much of an impact does this have? "
   ],
   "id": "22f0c242c2505f2b"
  },
  {
   "cell_type": "code",
   "id": "367abafc0ae13a6d",
   "metadata": {},
   "source": [
    "stretches_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8ee27ecc0697a5f5",
   "metadata": {},
   "source": [
    "# Perform the left merge\n",
    "double_curie_frame = double_curie_frame.merge(\n",
    "    stretches_df,\n",
    "    left_on='extracted_local_id_int',\n",
    "    right_on='value',\n",
    "    how='left'\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "59e278981b799f14",
   "metadata": {},
   "source": [
    "stretch_summary_df = summarize_stretch_groups(double_curie_frame)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For stretch 9, which included extracted CURIes from ENVO:01001458 to ENVO:01001511, the oaklib test annotation of 100% of the submitted environmental context annotations was ENVO:01001803, so we will keep that and disregard all of the CURIes from the stretch\n",
   "id": "46a141c88d2f24a8"
  },
  {
   "cell_type": "code",
   "id": "cee6e3ff1e1ca0d3",
   "metadata": {},
   "source": [
    "stretch_summary_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "decisive_fraction_threshold = 0.9",
   "id": "dcc5ba2adda9189f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "decisive_stretch_summary_df = stretch_summary_df[stretch_summary_df['fraction'] >= decisive_fraction_threshold]",
   "id": "529c6e4d71a8bf31",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "decisive_stretch_summary_df",
   "id": "db568a6f50b6e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f47a58a1945943b8",
   "metadata": {},
   "source": [
    "# Perform the left merge\n",
    "double_curie_frame = double_curie_frame.merge(\n",
    "    decisive_stretch_summary_df,\n",
    "    left_on='stretch_id',\n",
    "    right_on='stretch_id',\n",
    "    how='left'\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "double_curie_frame",
   "id": "babb21e01016715",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "828f3be5ac6c0ab1",
   "metadata": {},
   "source": [
    "drag_evidence_frame = double_curie_frame[double_curie_frame['stretch_id'] >= 1]\n",
    "drag_evidence_frame = drag_evidence_frame[['extracted_curie', 'longest_annotation_curie']]\n",
    "drag_evidence_frame['drag_evidence'] = True"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9907b1472af8731b",
   "metadata": {},
   "source": [
    "drag_evidence_frame"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9873eb1bc3ea44e5",
   "metadata": {},
   "source": [
    "ncbi_frame = ncbi_frame.merge(\n",
    "    drag_evidence_frame,\n",
    "    left_on=['extracted_curie', 'longest_annotation_curie'],\n",
    "    right_on=['extracted_curie', 'longest_annotation_curie'],\n",
    "    how='left'\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1d67b9e207b5531e",
   "metadata": {},
   "source": [
    "# Initialize dragless_curie_list with curie_list values\n",
    "ncbi_frame[\"dragless_curie_list\"] = ncbi_frame[\"curie_list\"]\n",
    "\n",
    "# Update dragless_curie_list based on the condition\n",
    "for index, row in ncbi_frame.iterrows():\n",
    "    if row[\"drag_evidence\"] is True:\n",
    "        if row[\"longest_annotation_curie\"] is not None:\n",
    "            ncbi_frame.at[index, \"dragless_curie_list\"] = [row[\"longest_annotation_curie\"]]\n",
    "        else:\n",
    "            ncbi_frame.at[index, \"dragless_curie_list\"] = []\n",
    "\n",
    "ncbi_frame['dragless_curie_count'] = ncbi_frame['dragless_curie_list'].apply(len)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fb7469cd6900edd7",
   "metadata": {},
   "source": [
    "ncbi_frame['unique_curie_count'].value_counts()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7626e42d5b746a2a",
   "metadata": {},
   "source": [
    "ncbi_frame['dragless_curie_count'].value_counts()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## The extent of multiple detected CURIes has been reduced ~ 4.5 fold \n",
    "(for soil env_local_scale)\n",
    "\n",
    "Isolate the submitter annotations for which there's clearly one best CURIe after removing the drag-stretches"
   ],
   "id": "55ba8eda9fc69708"
  },
  {
   "cell_type": "code",
   "id": "bb3c0eeb8e919ac5",
   "metadata": {},
   "source": [
    "ncbi_frame.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "653df3ac622d4e95",
   "metadata": {},
   "source": [
    "ncbi_frame_undisputed = ncbi_frame[ncbi_frame['dragless_curie_count'] <= 1]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2abdcbd301b484a9",
   "metadata": {},
   "source": [
    "ncbi_frame_undisputed.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "57d0c3c05e88ea1e",
   "metadata": {},
   "source": [
    "ncbi_frame_disputed = ncbi_frame[ncbi_frame['dragless_curie_count'] > 1]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d0cf6796fedc7f8a",
   "metadata": {},
   "source": [
    "ncbi_frame_disputed.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fe33fe1d6d2aad09",
   "metadata": {},
   "source": [
    "ncbi_frame_disputed = ncbi_frame_disputed.explode(\"dragless_curie_list\", ignore_index=True)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f8fe49b93cb12990",
   "metadata": {},
   "source": [
    "ncbi_frame_disputed.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6896ca8dd3865cb5",
   "metadata": {},
   "source": [
    "ncbi_frame_disputed[\"dragless_curie_list\"] = ncbi_frame_disputed[\"dragless_curie_list\"].apply(lambda x: [x])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Just include all of the remaining disputed CURIe assingments",
   "id": "849c8845f5c378e7"
  },
  {
   "cell_type": "code",
   "id": "9a472d228624f148",
   "metadata": {},
   "source": [
    "# Combine the rows of ncbi_frame_undisputed and ncbi_frame_disputed into a new DataFrame\n",
    "ncbi_disputes_exploded_frame = pd.concat([ncbi_frame_undisputed, ncbi_frame_disputed], ignore_index=True)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e772d6db19793ec5",
   "metadata": {},
   "source": [
    "ncbi_disputes_exploded_frame.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ef0b5e69d724d94a",
   "metadata": {},
   "source": [
    "ncbi_disputes_exploded_frame"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "be53a584475499b5",
   "metadata": {},
   "source": [
    "ncbi_disputes_exploded_frame['post_explode_curie_count'] = ncbi_disputes_exploded_frame['dragless_curie_list'].apply(len)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e0c0605a4ca28188",
   "metadata": {},
   "source": [
    "ncbi_disputes_exploded_frame['post_explode_curie_count'].value_counts()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "396e48f3cea80587",
   "metadata": {},
   "source": [
    "# Set 'post_explode_curie' to the 0th item in 'dragless_curie_list'\n",
    "ncbi_disputes_exploded_frame[\"post_explode_curie\"] = ncbi_disputes_exploded_frame[\"dragless_curie_list\"].apply(\n",
    "    lambda x: x[0] if isinstance(x, list) and len(x) > 0 else None\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1e488764b4212b75",
   "metadata": {},
   "source": [
    "\n",
    "ncbi_biosample_scoped_counts = (\n",
    "    ncbi_disputes_exploded_frame.groupby(\"post_explode_curie\")[\"sample_count\"].sum().reset_index()\n",
    ")\n",
    "\n",
    "ncbi_biosample_scoped_counts.columns = ['curie', 'ncbi_scoped_count']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This is currently a count of Biosamples for which the indicated CURIes can be extracted or inferred by oaklib annotation, after removal of drag-stretch, auto-incremented CURIes",
   "id": "de29981308f4d74c"
  },
  {
   "cell_type": "code",
   "id": "916cfcc3c7aa0d34",
   "metadata": {},
   "source": [
    "ncbi_biosample_scoped_counts"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## GOLD mappings/Biosample counts hybrid\n",
    "\n",
    "we're currently including\n",
    "- mappings in hybrid with biosample counts\n",
    "- mappings only\n",
    "\n",
    "And we're casting a wide net, especially for the hybrid approach\n",
    "- searching for 'soil', 'sediment' etc. in GOLDTERMS labels without anchoring them like 'Environmental > Aquatic > Sediment'\n",
    "- retrieving the CURIes for env_broad_scale, env_local_scale and env_medium for all voting sheets, and trusting orthogonal filtering to remove the inappropriate CURIes\n",
    "\n",
    "Should we now add (or switch to) direct biosample counts of GOLD \"envo\" annotations?\n",
    "\n",
    "Efficient retrieval of  all GOLD data in a given scope isn't easy"
   ],
   "id": "54ef0d1be4a1dea9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if os.path.isfile(gold_data_file_name):\n",
    "    print(f\"{gold_data_file_name} is already present in the current working directory.\")\n",
    "else:\n",
    "    print(f\"{gold_data_file_name} needs to be downloaded\")\n",
    "    gold_response = requests.get(gold_data_url)\n",
    "    with open(gold_data_file_name, \"wb\") as f:\n",
    "        f.write(gold_response.content)\n",
    "        # ~ 10 seconds  @ 250 Mbps"
   ],
   "id": "8de124964756a131",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Expect to see\n",
    "\n",
    "> /home/mark/.cache/pypoetry/virtualenvs/nmdc-submission-schema-DC6HKp4p-py3.10/lib/python3.10/site-packages/openpyxl/styles/stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
    "  warn(\"Workbook contains no default style, apply openpyxl's default\")"
   ],
   "id": "590bfd050233cda1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if os.path.isfile(gold_csv_file_name):\n",
    "    print(f\"{gold_csv_file_name} is present in the current working directory and will be parsed into gold_biosamples_frame.\")\n",
    "    gold_biosamples_frame = pd.read_csv(gold_csv_file_name, sep=\"\\t\")\n",
    "else:\n",
    "    print(f\"gold_biosamples_frame and {gold_csv_file_name} need to be created\")\n",
    "    gold_biosamples_frame = pd.read_excel(gold_data_file_name, sheet_name=BIOSAMPLES_SHEET)\n",
    "    gold_biosamples_frame.to_csv(\"gold_biosamples.csv\", index=False, sep=\"\\t\")\n",
    "    # 2 minutes"
   ],
   "id": "b2bded0c859cfdb7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "gold_biosamples_frame['BIOSAMPLE ECOSYSTEM PATH ID'] = gold_biosamples_frame['BIOSAMPLE ECOSYSTEM PATH ID'].fillna(\n",
    "    0).astype(int)\n"
   ],
   "id": "fb292efb113d881a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "gold_biosamples_frame",
   "id": "ca3110d5a0a1fa80",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Determine the filenames and target directory\n",
    "goldterms_compressed_filename = urlparse(goldterms_semsql_url).path.split('/')[-1]\n",
    "goldterms_filename = os.path.splitext(goldterms_compressed_filename)[0]\n",
    "target_dir = os.path.join(\"..\", \"..\")  # Two levels up\n",
    "\n",
    "# Print to confirm the filenames\n",
    "print(goldterms_filename)"
   ],
   "id": "40f8fc6862a817f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Fetch the contents from the URL and save compressed file in target directory\n",
    "goldterms_response = requests.get(goldterms_semsql_url)\n",
    "goldterms_compressed_file_path = os.path.join(target_dir, goldterms_compressed_filename)\n",
    "with open(goldterms_compressed_file_path, \"wb\") as f:\n",
    "    f.write(goldterms_response.content)\n",
    "    \n",
    "# ~ 1 second"
   ],
   "id": "784c07d7b6bee4c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Unzip the compressed file and save the extracted file in target directory\n",
    "goldterms_uncompressed_file_path = os.path.join(target_dir, goldterms_filename)\n",
    "with gzip.open(goldterms_compressed_file_path, \"rb\") as f_in:\n",
    "    with open(goldterms_uncompressed_file_path, \"wb\") as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "# ~ 1 second"
   ],
   "id": "cd765130d59cd026",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# that's all fast. don't bother caching",
   "id": "fd2fab3ddcf862ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "goldterms_conn = sqlite3.connect(goldterms_uncompressed_file_path)",
   "id": "dcf847a3c2ed302d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "goldterms_subjects = pd.read_sql_query(goldterms_subclass_query, goldterms_conn)",
   "id": "cd5b0875ee9410c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "goldterms_subjects['path_id'] = goldterms_subjects['subject'].str.extract(r'GOLDTERMS:(\\d+)')",
   "id": "b35003035f6c1833",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "goldterms_subjects",
   "id": "afaaf46c3a83dc3e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "gold_path_ids = goldterms_subjects['path_id'].dropna().unique().tolist()\n",
    "gold_path_ids = [int(my_id) for my_id in gold_path_ids]\n"
   ],
   "id": "6d8f2db9770e83b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "gold_env_filtered_biosamples_frame = gold_biosamples_frame[\n",
    "    gold_biosamples_frame['BIOSAMPLE ECOSYSTEM PATH ID'].isin(gold_path_ids)]\n"
   ],
   "id": "2d78193b734e2ab4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "gold_env_filtered_biosamples_frame",
   "id": "3f24e8a82845de52",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "goldterms_context_frame = pd.read_sql_query(goldterms_envo_query, goldterms_conn)",
   "id": "22c7f8e126e96aab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "goldterms_context_frame['object_label'] = goldterms_context_frame['object'].apply(envo_adapter.label)",
   "id": "3157b066b1c6e2cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "goldterms_context_frame['path_id'] = goldterms_context_frame['subject'].str.extract(r'GOLDTERMS:(\\d+)')",
   "id": "d60306a6490fdd6a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "goldterms_context_frame",
   "id": "93391b53436ba5b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Fill NaN values in 'BIOSAMPLE ECOSYSTEM PATH ID' with 0 and convert to int\n",
    "gold_env_filtered_biosamples_frame['BIOSAMPLE ECOSYSTEM PATH ID'] = gold_env_filtered_biosamples_frame[\n",
    "    'BIOSAMPLE ECOSYSTEM PATH ID'].fillna(0).astype(int)\n",
    "\n",
    "# Drop rows with NaN in 'path_id' in goldterms_context_frame\n",
    "goldterms_context_frame = goldterms_context_frame.dropna(subset=['path_id'])\n",
    "\n",
    "# Convert 'path_id' to int\n",
    "goldterms_context_frame['path_id'] = goldterms_context_frame['path_id'].astype(int)\n",
    "\n",
    "# Perform the left merge\n",
    "gold_env_filtered_biosamples_with_inferred = gold_env_filtered_biosamples_frame.merge(\n",
    "    goldterms_context_frame,\n",
    "    left_on='BIOSAMPLE ECOSYSTEM PATH ID',\n",
    "    right_on='path_id',\n",
    "    how='left'\n",
    ")\n"
   ],
   "id": "beb2e0a9c669a366",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "gold_env_filtered_biosamples_with_inferred",
   "id": "bac06360f067d3c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# GOLDTERMS only approach",
   "id": "8c66c02b62dbcf22"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "goldterms_result = pd.read_sql_query(extension_query, goldterms_conn)",
   "id": "beacc603e5db48b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "goldterms_result",
   "id": "85e373122e14f6dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # todo: save this kind of content before subsetting on an environment\n",
    "# #   the subsetting is currently baked into the query\n",
    "# \n",
    "# # see also goldterms_queries.ipynb in MAM's Collab\n",
    "# goldterms_result.to_csv(\"goldterms_single_environment_mappings_long.tsv\", sep=\"\\t\", index=False)"
   ],
   "id": "859a87109b08d83c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "goldterms_only_curies = goldterms_result.loc[goldterms_result['predicate'].isin(gold_context_selectors), 'content']\n",
   "id": "391d1fb9314d4b0b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "goldterms_only_curies = goldterms_only_curies.unique().tolist()",
   "id": "b5621477ccaaa761",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# goldterms_only_curies",
   "id": "70238d1c8e144cc3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Make lists of CURIEs\n",
    "which will determine\n",
    "- the rows in the table\n",
    "- the boolean filter columns"
   ],
   "id": "8541580a525201e3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "anchor_curies = list(anchor_descendants_frame['curie'])\n",
    "legacy_pv_curies = [i['curie'] for i in pv_validation_results['valids']]\n",
    "\n",
    "biome_curies = list(envo_adapter.descendants(BIOME, predicates=[IS_A])) # \n",
    "terrestrial_biome_curies = list(envo_adapter.descendants(TERRESTRIAL_BIOME, predicates=[IS_A]))\n",
    "aquatic_biome_curies = list(envo_adapter.descendants(AQUATIC_BIOME, predicates=[IS_A]))\n",
    "abp_curies = list(envo_adapter.descendants(ABP, predicates=[IS_A]))\n",
    "env_sys_curies = list(envo_adapter.descendants(ENVIRONMENTAL_SYSTEM, predicates=[IS_A]))\n",
    "env_mat_curies = list(envo_adapter.descendants(ENVIRONMENTAL_MATERIAL, predicates=[IS_A]))\n",
    "obsoletes_curies = list(envo_adapter.obsoletes())\n",
    "\n",
    "soil_curies = list(envo_adapter.descendants(SOIL, predicates=[IS_A])) # \n",
    "liquid_water_curies = list(envo_adapter.descendants(LIQUID_WATER, predicates=[IS_A])) # \n",
    "water_ice_curies = list(envo_adapter.descendants(WATER_ICE, predicates=[IS_A])) # \n",
    "\n",
    "human_construction_curies = list(envo_adapter.descendants(HUMAN_CONSTRUCTION, predicates=[IS_A])) #\n",
    "building_curies = list(envo_adapter.descendants(BUILDING, predicates=[IS_A])) #\n",
    "building_part_curies = list(envo_adapter.descendants(BUILDING_PART, predicates=[IS_A])) #\n"
   ],
   "id": "beb69545a9255099",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Bootstrap the rows",
   "id": "1318a3f163d8cb19"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "include_in_rows = set()",
   "id": "20a9dcc52b995762",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "include_in_rows.update(anchor_curies)",
   "id": "2c3e280c36ed9b91",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "include_in_rows.update(legacy_pv_curies)",
   "id": "422c62627b450632",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "include_in_rows.update(nmdc_biosample_contexts_frame[nmdc_context_selector])",
   "id": "e279a2edf1fa4935",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "include_in_rows.update(ncbi_frame['extracted_curie'])",
   "id": "7e4be4b7ae64e990",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "include_in_rows.update(ncbi_frame['longest_annotation_curie'])",
   "id": "2cd900624cfce1ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "include_in_rows.update(gold_env_filtered_biosamples_with_inferred['object'])",
   "id": "9790ec48f41afe79",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "include_in_rows.update(goldterms_only_curies)",
   "id": "31ff1c10df555677",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "rows_lod = []",
   "id": "f7a6fb9413f45298",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Begin constructing the voting sheet",
   "id": "d0c9c7c730c8308a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for curie in include_in_rows:\n",
    "    if curie is None:\n",
    "        continue\n",
    "        \n",
    "    # ONCE AGAIN, assuming that EnvO is the only ontology we'll check against\n",
    "    current_ancestors = list(envo_adapter.ancestors(curie, predicates=[IS_A])) # vs legacy_pv_curies\n",
    "    ancestors_in_enum_count = len(set(current_ancestors) & set(legacy_pv_curies))\n",
    "    \n",
    "    current_descendants  = list(envo_adapter.descendants(curie, predicates=[IS_A])) # vs legacy_pv_curies\n",
    "    descendants_in_enum_count  = len(set(current_descendants) & set(legacy_pv_curies))\n",
    "    \n",
    "    \n",
    "    row = {\n",
    "        'curie': curie,\n",
    "        'label': envo_adapter.label(curie),\n",
    "        'envo_native': False,\n",
    "        'obsolete': False,\n",
    "        comparison_enum_column_name: False,\n",
    "        'ancestors_in_enum_count': ancestors_in_enum_count,\n",
    "        'descendants_in_enum_count': descendants_in_enum_count,\n",
    "        'abp': False,\n",
    "        'env_sys': False,\n",
    "        'biome': False,\n",
    "        'terrestrial_biome': False,\n",
    "        'aquatic_biome': False,\n",
    "        'env_mat': False,\n",
    "        'soil': False,\n",
    "        'liquid water': False,\n",
    "        'water ice': False,\n",
    "        'human_construction': False,\n",
    "        'building': False,\n",
    "        'building_part': False,\n",
    "        'goldterms_mappings': False,\n",
    "    }\n",
    "        \n",
    "    if curie in biome_curies:\n",
    "        row['biome'] = True\n",
    "    if curie in terrestrial_biome_curies:\n",
    "        row['terrestrial_biome'] = True\n",
    "    if curie in aquatic_biome_curies:\n",
    "        row['aquatic_biome'] = True\n",
    "    if curie in abp_curies:\n",
    "        row['abp'] = True\n",
    "    if curie in env_sys_curies:\n",
    "        row['env_sys'] = True\n",
    "    if curie in env_mat_curies:\n",
    "        row['env_mat'] = True\n",
    "    if curie in soil_curies:\n",
    "        row['soil'] = True\n",
    "    if curie in liquid_water_curies:\n",
    "        row['liquid water'] = True\n",
    "    if curie in water_ice_curies:\n",
    "        row['water ice'] = True\n",
    "    if curie in human_construction_curies:\n",
    "        row['human_construction'] = True\n",
    "    if curie in building_curies:\n",
    "        row['building'] = True\n",
    "    if curie in building_part_curies:\n",
    "        row['building_part'] = True\n",
    "    if curie in legacy_pv_curies:\n",
    "        row[comparison_enum_column_name] = True\n",
    "    if curie in obsoletes_curies:\n",
    "        row['obsolete'] = True\n",
    "    if curie in goldterms_only_curies:\n",
    "        row['goldterms_mappings'] = True\n",
    "        \n",
    "    try:\n",
    "        prefix, local_id = curie.split(':')\n",
    "        if prefix and prefix == 'ENVO' and row['label'] is not None:\n",
    "            row['envo_native'] = True\n",
    "    except Exception as e:\n",
    "        # Print the exception message\n",
    "        print(f\"An error occurred: {e} trying to split {curie}\")\n",
    "\n",
    "    rows_lod.append(row)\n",
    "\n",
    "# 2 minutes\n"
   ],
   "id": "32f278db05c57616",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ^ Construction of the voting sheet",
   "id": "f77fce51b7fd116d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "rows_frame = pd.DataFrame(rows_lod)",
   "id": "26bb62d21192288f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "rows_frame",
   "id": "4db3ebadb8a07a50",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Merge in NMDC counts",
   "id": "8f7c4b1d95722568"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "nmdc_biosample_scoped_counts = nmdc_biosample_contexts_frame[nmdc_context_selector].value_counts().reset_index()\n",
    "nmdc_biosample_scoped_counts.columns = ['curie', 'nmdc_scoped_count']\n"
   ],
   "id": "88ebf6f76c2e0545",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "nmdc_biosample_scoped_counts",
   "id": "85789ece9ec4ddd6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Perform the left merge\n",
    "rows_frame = rows_frame.merge(\n",
    "    nmdc_biosample_scoped_counts,\n",
    "    left_on='curie',\n",
    "    right_on='curie',\n",
    "    how='left'\n",
    ")"
   ],
   "id": "329203aecb25f598",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Merge in GOLD hybrid counts",
   "id": "f77f59f09d479391"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "gold_env_filtered_biosamples_with_inferred",
   "id": "cff020dba8153cb2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "gold_biosample_scoped_counts = gold_env_filtered_biosamples_with_inferred['object'].value_counts().reset_index()\n",
    "gold_biosample_scoped_counts.columns = ['curie', 'gold_hybrid_count']"
   ],
   "id": "332592b73d143dc0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "gold_biosample_scoped_counts",
   "id": "57d16757b67e19a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Perform the left merge\n",
    "rows_frame = rows_frame.merge(\n",
    "    gold_biosample_scoped_counts,\n",
    "    left_on='curie',\n",
    "    right_on='curie',\n",
    "    how='left'\n",
    ")"
   ],
   "id": "65ab8871a20a93d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "rows_frame",
   "id": "8346fb59af8e61c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# gold and ncbi counts are slightly trickier\n",
    "# for gold: including mappings only, mappings in hybrid with biosample counts. \n",
    "#    Switch to direct biosample counts of GOLD \"envo\" annotations?\n",
    "# ncbi: we have extracted curies and annotated curies"
   ],
   "id": "87a64ed7d57d81ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Perform the left merge\n",
    "rows_frame = rows_frame.merge(\n",
    "    ncbi_biosample_scoped_counts,\n",
    "    left_on='curie',\n",
    "    right_on='curie',\n",
    "    how='left'\n",
    ")"
   ],
   "id": "f4d8dc1053a5bb7e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# 990 rows in https://docs.google.com/spreadsheets/d/12WH3eduBq2qSTy9zVF3n7fyajn6ssLZL/edit?gid=546570706#gid=546570706",
   "id": "396c3f6005a31874",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "rows_frame.to_csv(output_file_name, sep=\"\\t\", index=False)",
   "id": "b7a26a1b879188a4",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
