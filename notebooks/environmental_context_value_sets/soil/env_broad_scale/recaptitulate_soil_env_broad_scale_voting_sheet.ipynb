{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from operator import index\n",
    "\n",
    "from oaklib import get_adapter\n",
    "from oaklib.datamodels.vocabulary import IS_A, PART_OF\n",
    "import pandas as pd\n",
    "from linkml_runtime import SchemaView\n",
    "from linkml_runtime.dumpers import yaml_dumper\n",
    "import requests\n",
    "import pprint\n",
    "\n",
    "from typing import Dict, Any\n",
    "import csv\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer  # from scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "import re\n",
    "\n",
    "import os\n",
    "\n",
    "import duckdb\n",
    "import sqlite3\n",
    "\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "from urllib.parse import urlparse"
   ],
   "id": "630126a7790cc3a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Approved prefixes (case-insensitive)\n",
    "approved_prefixes = ['ENVO']"
   ],
   "id": "914fdae0d02dad58",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# make a biomes curie -> label dict?\n",
    "BIOME_CURIE = 'ENVO:00000428'"
   ],
   "id": "95f7efc06f20987a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ENV_BROAD_SCALE_ENUM = \"EnvBroadScaleSoilEnum\"",
   "id": "20750df32ebb2033",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ncbi_query = \"\"\"\n",
    "SELECT content, COUNT(1) AS sample_count \n",
    "FROM attributes \n",
    "WHERE harmonized_name = 'env_broad_scale' AND package_content = 'MIMS.me.soil.6.0' \n",
    "GROUP BY content \n",
    "ORDER BY COUNT(1) DESC\n",
    "\"\"\""
   ],
   "id": "d33879ec4c6ca1df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "envo_adapter_string = \"sqlite:obo:envo\"",
   "id": "8240c0de4cbcec24",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# goldterms_adapter_string = \"sqlite:obo:envo\"",
   "id": "eb27b996bb24645a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "goldterms_semsql_url = \"https://s3.amazonaws.com/bbop-sqlite/goldterms.db.gz\"\n",
    "\n",
    "# https://s3.amazonaws.com/bbop-sqlite/\n",
    "# <Contents>\n",
    "# <Key>goldterms.db.gz</Key>\n",
    "# <LastModified>2024-11-03T17:24:56.000Z</LastModified>\n",
    "# <ETag>\"fe8e35b215786cb9fc347b7fadbe055f\"</ETag>\n",
    "# <Size>2935781</Size>\n",
    "# <StorageClass>STANDARD</StorageClass>\n",
    "# </Contents>\n"
   ],
   "id": "2ae64976b48e359",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# todo could this have been done with a OAK query, eliminating the need to explicitly download the file?\n",
    "\n",
    "goldterms_envo_query = \"\"\"\n",
    "SELECT\n",
    "\t*\n",
    "FROM\n",
    "\tstatements s\n",
    "WHERE\n",
    "\tpredicate = 'mixs:env_broad'\"\"\""
   ],
   "id": "ae751e002f65be71",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "GOLDTERMS_SOIL = 'GOLDTERMS:4212'",
   "id": "eda591e14a91cd01",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "goldterms_soil_subclass_query = f\"\"\"\n",
    "select\n",
    "\tsubject\n",
    "from\n",
    "\tentailed_edge ee\n",
    "where\n",
    "\tpredicate = 'rdfs:subClassOf'\n",
    "\tand object = '{GOLDTERMS_SOIL}'\n",
    "\"\"\""
   ],
   "id": "6b672d93e1f5c53",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "previous_submission_schema_url = \"https://raw.githubusercontent.com/microbiomedata/submission-schema/v10.7.0/src/nmdc_submission_schema/schema/nmdc_submission_schema.yaml\"",
   "id": "a1e9e99c5ac41ef4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "NMDC_RUNTIME_BASE_URL = 'https://api.microbiomedata.org/nmdcschema/'\n",
    "STUDY_SET_COLLECTION = 'study_set'\n",
    "BIOSAMPLE_SET_COLLECTION = 'biosample_set'"
   ],
   "id": "cde9a57dca22a7c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "env_package_override_file = '../../mam-env-package-overrides.tsv'\n",
    "override_column = 'mam_inferred_env_package'"
   ],
   "id": "375bac6b84ab6198",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ncbi_duckdb_file = '../../ncbi_biosamples.duckdb'\n",
    "\n",
    "ncbi_duckdb_url = 'https://portal.nersc.gov/project/m3408/biosamples_duckdb/ncbi_biosamples_2024-09-23.duckdb.gz'"
   ],
   "id": "8a8be3a9e6123df5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "gold_data_url = \"https://gold.jgi.doe.gov/download?mode=site_excel\"\n",
    "BIOSAMPLES_SHEET = \"Biosample\""
   ],
   "id": "fb9c800afcc86146",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initialize cache dictionaries for predict_from_normalized_env_packages\n",
    "ancestor_cache = {}\n",
    "descendant_cache = {}"
   ],
   "id": "f952b5888d2c5f0d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# todo is filling memory with things like this a good idea? for understandability? or performance?\n",
    "# todo they should be aggregated somewhere, as specified by the config.yaml\n",
    "# todo or should we going straight to data frames? in which case a dlist of dicts might be preferable\n",
    "def get_curie_descendants_label_dict(curie, predicates, adapter):\n",
    "    curie_label_dict = {}\n",
    "    for descendant in adapter.descendants(curie, predicates=predicates):\n",
    "        curie_label_dict[descendant] = adapter.label(descendant)\n",
    "    return curie_label_dict"
   ],
   "id": "342c71e8b7a3366f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def curie_descendants_label_dict_to_lod(curie_label_dict):\n",
    "    return [{'curie': k, 'label': v} for k, v in curie_label_dict.items()]"
   ],
   "id": "1f4304edb0904f25",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def curie_descendants_label_lod_to_df(curie_label_lod):\n",
    "    return pd.DataFrame(curie_label_lod)"
   ],
   "id": "c43fc53d70ff43e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_schemaview_from_source(source):\n",
    "    return SchemaView(source)"
   ],
   "id": "28e160b747e7480e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# def get_schema_from_schemaview(schemaview):\n",
    "#     return schemaview.schema"
   ],
   "id": "993d01723365ec07",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def parse_hierarchically_underscored_strings(hierarchically_underscored_string_list):\n",
    "    result = []\n",
    "    for item in hierarchically_underscored_string_list:\n",
    "        # Remove leading underscores for label, split on '[' to separate curie\n",
    "        label, curie = item.lstrip('_').split(' [')\n",
    "        # Remove the trailing ']' from curie\n",
    "        curie = curie.rstrip(']')\n",
    "        # Append dictionary with label and curie\n",
    "        result.append({'label': label.strip(), 'curie': curie.strip()})\n",
    "    return result"
   ],
   "id": "8ce34ecfbae82a23",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def dedupe_underscoreless_pvs(underscoreless_pvs):\n",
    "    # Dictionary to store CURIE as key and list of unique labels as values\n",
    "    curie_to_labels = {}\n",
    "\n",
    "    for item in underscoreless_pvs:\n",
    "        curie = item['curie']\n",
    "        label = item['label']\n",
    "\n",
    "        # Initialize the list if curie is not yet a key\n",
    "        if curie not in curie_to_labels:\n",
    "            curie_to_labels[curie] = []\n",
    "\n",
    "        # Add label if it is not already in the list for this curie\n",
    "        if label not in curie_to_labels[curie]:\n",
    "            curie_to_labels[curie].append(label)\n",
    "    return curie_to_labels\n"
   ],
   "id": "ce2b4b74d1fdc0d4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def validate_curie_label_list_dict(curie_label_dict, adapter, print_flag=False):\n",
    "    problem_curies = []\n",
    "    valid_curies = []\n",
    "    for curie, labels in curie_label_dict.items():\n",
    "        true_label = adapter.label(curie)\n",
    "        if true_label not in labels:\n",
    "            problem_curies.append(curie)\n",
    "            if print_flag:\n",
    "                print(f\"Error: {curie} has true label {true_label} which doesn't appear in {labels}\")\n",
    "        else:\n",
    "            valid_curies.append({\"curie\": curie, \"label\": true_label})\n",
    "    return {\"problems\": problem_curies, \"valids\": valid_curies}"
   ],
   "id": "cbd3ed3aec38ee16",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# todo could pre-determine the collection sizes\n",
    "# todo could report elapsed time\n",
    "\n",
    "def get_docs_from_nmdc_collection(base_url, collection_name, max_page_size=1000, stop_after=None):\n",
    "    \"\"\"\n",
    "    Fetch all documents from a paginated API. Defaults to fetching a large number of documents per page.\n",
    "    Optionally stop after a specified number of documents.\n",
    "\n",
    "    Parameters:\n",
    "    - base_url: The base URL of the API endpoint (e.g., 'https://api.microbiomedata.org/nmdcschema/').\n",
    "    - collection_name: The name of the collection to fetch (e.g., 'biosample_set').\n",
    "    - max_page_size: The maximum number of items to retrieve per page (default 1000).\n",
    "    - stop_after: Optional parameter to stop fetching after a certain number of documents (default None).\n",
    "\n",
    "    Returns:\n",
    "    - A list of documents fetched from the API.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    page_token = None\n",
    "    total_documents = 0\n",
    "    page_count = 0\n",
    "\n",
    "    # Construct the full URL with the collection name\n",
    "    url = f\"{base_url}{collection_name}\"\n",
    "\n",
    "    while True:\n",
    "        page_count += 1\n",
    "        # Prepare the query parameters\n",
    "        params = {\n",
    "            'collection_name': collection_name,\n",
    "            'max_page_size': max_page_size,  # Set large max_page_size to reduce pagination\n",
    "        }\n",
    "\n",
    "        if page_token:\n",
    "            params['page_token'] = page_token  # Add the page token for pagination\n",
    "\n",
    "        # Send the request to the API\n",
    "        response = requests.get(url, params=params)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error fetching data: {response.status_code}\")\n",
    "            break\n",
    "\n",
    "        data = response.json()\n",
    "\n",
    "        # Add the current page of documents to the list\n",
    "        num_documents_on_page = len(data['resources'])\n",
    "        documents.extend(data['resources'])\n",
    "        total_documents += num_documents_on_page\n",
    "\n",
    "        # Status reporting\n",
    "        print(f\"Fetched page {page_count} with {num_documents_on_page} documents. Total fetched: {total_documents}\")\n",
    "\n",
    "        # If stop_after is provided, stop fetching after reaching the specified number of documents\n",
    "        if stop_after and total_documents >= stop_after:\n",
    "            documents = documents[:stop_after]  # Trim to the required number\n",
    "            print(f\"Reached stop_after limit of {stop_after} documents.\")\n",
    "            break\n",
    "\n",
    "        # Check if there is a next page\n",
    "        page_token = data.get('next_page_token')\n",
    "        if not page_token:\n",
    "            print(\"All documents fetched.\")\n",
    "            break  # Exit the loop if no more pages are available\n",
    "\n",
    "    return documents"
   ],
   "id": "69ea6069b4c4abbb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "def get_name_or_rawval(env_scale: Dict[str, Any]) -> str:\n",
    "    \"\"\"Safely extract label from environmental scale data.\"\"\"\n",
    "    if env_scale:\n",
    "        term = env_scale.get('term')\n",
    "        if term:\n",
    "            return term.get('name', term.get('has_raw_value', ''))\n",
    "    return ''"
   ],
   "id": "590502abe99de710",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def tsv_to_dict_of_dicts(tsv_file, outer_key_column):\n",
    "    \"\"\"\n",
    "    Reads a TSV file into a dictionary of dictionaries.\n",
    "\n",
    "    :param tsv_file: Path to the TSV file.\n",
    "    :param outer_key_column: The column name or index to be used as the key for the outer dictionary.\n",
    "    :return: A dictionary of dictionaries, with outer keys being the values from the specified column.\n",
    "    \"\"\"\n",
    "    with open(tsv_file, newline='', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f, delimiter='\\t')\n",
    "\n",
    "        result = {}\n",
    "\n",
    "        for row in reader:\n",
    "            outer_key = row[outer_key_column]\n",
    "            result[outer_key] = {key: value for key, value in row.items() if key != outer_key_column}\n",
    "\n",
    "    return result"
   ],
   "id": "6668e06afbd16f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# todo only gets authoritative labels from the passed adapter, which is presumably EnvO only\n",
    "# todo would benefit from caching of labels\n",
    "\n",
    "def biosamples_lod_context_extractor(biosamples_lod, adapter, env_pacakge_overrides=None):\n",
    "    new_lod = []\n",
    "    for biosample in biosamples_lod:\n",
    "        insdc_identifiers = biosample.get('insdc_biosample_identifiers', [])\n",
    "\n",
    "        env_broad_scale_label = get_name_or_rawval(biosample.get('env_broad_scale'))\n",
    "        env_local_scale_label = get_name_or_rawval(biosample.get('env_local_scale'))\n",
    "        env_medium_label = get_name_or_rawval(biosample.get('env_medium'))\n",
    "\n",
    "        # Extracting optional scalar env_package.has_raw_value\n",
    "        env_package_has_raw_value = biosample.get('env_package', {}).get('has_raw_value', '')\n",
    "\n",
    "        # Extracting required multivalued part_of\n",
    "        associated_studies = '|'.join(biosample.get('associated_studies', []))  # Assuming part_of is a list of strings\n",
    "\n",
    "        row: Dict[str, str] = {\n",
    "            'id': biosample['id'],\n",
    "            'insdc_biosample_identifiers': '|'.join(insdc_identifiers) if insdc_identifiers else '',\n",
    "\n",
    "            'env_broad_scale_id': biosample['env_broad_scale']['term']['id'],\n",
    "            'env_broad_scale_mongo_label': env_broad_scale_label,\n",
    "            'env_broad_scale_auth_label': adapter.label(biosample['env_broad_scale']['term']['id']),\n",
    "\n",
    "            'env_local_scale_id': biosample['env_local_scale']['term']['id'],\n",
    "            'env_local_scale_mongo_label': env_local_scale_label,\n",
    "            'env_local_scale_auth_label': adapter.label(biosample['env_local_scale']['term']['id']),\n",
    "\n",
    "            'env_medium_id': biosample['env_medium']['term']['id'],\n",
    "            'env_medium_mongo_label': env_medium_label,\n",
    "            'env_medium_auth_label': adapter.label(biosample['env_medium']['term']['id']),\n",
    "\n",
    "            'env_package_has_raw_value': env_package_has_raw_value,\n",
    "            'normalized_env_package': 'soil' if env_package_has_raw_value == 'ENVO:00001998' else env_package_has_raw_value.lower(),\n",
    "            # todo abstract this though label search, or at least providing a lookup structure\n",
    "\n",
    "            'associated_studies': associated_studies\n",
    "        }\n",
    "\n",
    "        if env_pacakge_overrides and biosample['id'] in env_pacakge_overrides:\n",
    "            print(\n",
    "                f\"Overriding env_package for biosample {biosample['id']} from {row['normalized_env_package']} to {env_pacakge_overrides[biosample['id']]['mam_inferred_env_package']}\")\n",
    "            row['normalized_env_package'] = env_pacakge_overrides[biosample['id']]['mam_inferred_env_package']\n",
    "\n",
    "        new_lod.append(row)\n",
    "    return new_lod\n"
   ],
   "id": "53b9c7e7d15845f6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_hierarchy_terms(curie: str, adapter) -> dict:\n",
    "    \"\"\"\n",
    "    Extract ancestor and descendant terms from the ontology for a given CURIE,\n",
    "    using caching to improve performance and filtering by 'is_a' relationships.\n",
    "\n",
    "    Args:\n",
    "        curie (str): CURIE identifier for the ontology term.\n",
    "        adapter: Ontology adapter.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing lists of ancestor and descendant terms.\n",
    "    \"\"\"\n",
    "    if curie in ancestor_cache:\n",
    "        ancestors = ancestor_cache[curie]\n",
    "    else:\n",
    "        try:\n",
    "            ancestors = list(adapter.ancestors(curie, predicates=[IS_A]))\n",
    "            ancestor_cache[curie] = [adapter.label(ancestor) for ancestor in ancestors if ancestor]\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving ancestors for {curie}: {e}\")\n",
    "            ancestor_cache[curie] = []\n",
    "\n",
    "    if curie in descendant_cache:\n",
    "        descendants = descendant_cache[curie]\n",
    "    else:\n",
    "        try:\n",
    "            descendants = list(adapter.descendants(curie, predicates=[IS_A]))\n",
    "            descendant_cache[curie] = [adapter.label(descendant) for descendant in descendants if descendant]\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving descendants for {curie}: {e}\")\n",
    "            descendant_cache[curie] = []\n",
    "\n",
    "    return {\n",
    "        'ancestors': ancestor_cache[curie],\n",
    "        'descendants': descendant_cache[curie],\n",
    "    }"
   ],
   "id": "3efcfc9fcc2fe391",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def vectorize_terms(df, column):\n",
    "    \"\"\"\n",
    "    Vectorize the ancestor or descendant terms for a given column.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input dataframe.\n",
    "        column (str): The column name to vectorize.\n",
    "\n",
    "    Returns:\n",
    "        sparse matrix: The vectorized term matrix.\n",
    "    \"\"\"\n",
    "    vectorizer = CountVectorizer()\n",
    "    return vectorizer.fit_transform(\n",
    "        df[column].apply(lambda x: ' '.join([str(term) for term in x if term is not None]) if x is not None else '')\n",
    "    )"
   ],
   "id": "e9be0a2e69ee7409",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def predict_from_normalized_env_packages(df_raw, adapter):\n",
    "    # Apply the function to the relevant columns\n",
    "\n",
    "    df = df_raw.copy()\n",
    "    for column in ['env_broad_scale_id', 'env_local_scale_id', 'env_medium_id']:\n",
    "        df[f'{column}_ancestors'] = df[column].apply(lambda x: get_hierarchy_terms(x, adapter)['ancestors'])\n",
    "        df[f'{column}_descendants'] = df[column].apply(lambda x: get_hierarchy_terms(x, adapter)['descendants'])\n",
    "\n",
    "    # Vectorize each set of terms separately\n",
    "    broad_scale_ancestors = vectorize_terms(df, 'env_broad_scale_id_ancestors')\n",
    "    broad_scale_descendants = vectorize_terms(df, 'env_broad_scale_id_descendants')\n",
    "\n",
    "    local_scale_ancestors = vectorize_terms(df, 'env_local_scale_id_ancestors')\n",
    "    local_scale_descendants = vectorize_terms(df, 'env_local_scale_id_descendants')\n",
    "\n",
    "    medium_ancestors = vectorize_terms(df, 'env_medium_id_ancestors')\n",
    "    medium_descendants = vectorize_terms(df, 'env_medium_id_descendants')\n",
    "\n",
    "    # Combine all feature matrices\n",
    "    X = hstack([\n",
    "        broad_scale_ancestors,\n",
    "        broad_scale_descendants,\n",
    "        local_scale_ancestors,\n",
    "        local_scale_descendants,\n",
    "        medium_ancestors,\n",
    "        medium_descendants\n",
    "    ])\n",
    "\n",
    "    # Filter the DataFrame to only include non-null rows for the target column\n",
    "    df_filtered = df[df['normalized_env_package'].notnull() & (df['normalized_env_package'] != \"\")]\n",
    "\n",
    "    # Extract the target variable\n",
    "    y = df_filtered['normalized_env_package']\n",
    "\n",
    "    # Ensure X corresponds to the filtered rows\n",
    "    X_filtered = X[df_filtered.index]\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_filtered, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Train a Random Forest Classifier\n",
    "    clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # # Predict the normalized_env_package for all rows\n",
    "    # df['predicted_normalized_env_package'] = clf.predict(X)\n",
    "\n",
    "    # # If you want to add confidence scores for each class\n",
    "    # class_probabilities = clf.predict_proba(X)\n",
    "    # \n",
    "    # # Get the class labels from the model\n",
    "    # class_labels = clf.classes_\n",
    "    # \n",
    "    # # Add a column for each class with the corresponding confidence score\n",
    "    # for i, class_label in enumerate(class_labels):\n",
    "    #     df[f'confidence_{class_label}'] = class_probabilities[:, i]\n",
    "    # \n",
    "    # return df\n",
    "\n",
    "    return clf.predict(X)"
   ],
   "id": "f03bb583dfde6d48",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def parse_curie_label(text, approved_prefixes=['ENVO']):\n",
    "    # Case-insensitive pattern for matching approved prefixes followed by an ID\n",
    "    pattern = r'\\b(?:' + '|'.join(approved_prefixes) + r')\\s*[:_]\\s*(\\d+)\\b'\n",
    "    curie_match = re.search(pattern, text, re.IGNORECASE)\n",
    "    \n",
    "    if curie_match:\n",
    "        curie = f\"{approved_prefixes[0].upper()}:{curie_match.group(1)}\"  # standardize prefix to 'ENVO:ID'\n",
    "        label = re.sub(pattern, \"\", text).strip(\"[]() \")\n",
    "        # replace any colons in the label with a whitespace\n",
    "        return pd.Series([label, curie])\n",
    "    else:\n",
    "        label = re.sub(r':', ' ', text)\n",
    "        return pd.Series([label, None])  # No CURIE found, return original label and None for CURIE\n"
   ],
   "id": "9edf0d5477083503",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_longest_annotation_curie(text, adapter):\n",
    "    annotations = adapter.annotate_text(text)\n",
    "    if not annotations:  # Check if annotations list is empty\n",
    "        return None\n",
    "    try:\n",
    "        longest_annotation = max(annotations, key=lambda x: x.subject_end - x.subject_start)\n",
    "        return longest_annotation.object_id\n",
    "    except ValueError:\n",
    "        return None  # Return None if there's an unexpected issue with finding the max\n"
   ],
   "id": "8db01eb510b460c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Can this be reconstructed in a traceable and reusable manner quickly? \n",
    "\n",
    "If so, then reuse this for generating water and sediment broad scale voting sheets.\n",
    "\n",
    "See https://github.com/microbiomedata/external-metadata-awareness/blob/main/Makefiles/soil-env_broad_scale.Makefile\n",
    "\n",
    "and all of it's dependencies\n",
    "\n",
    "**or this newer, un-merged version**\n",
    "\n",
    "```makefile\n",
    "local/soil-env-broad-scale-evidence-table.tsv: config/soil-env_broad_scale-evidence-config.yaml \\\n",
    "local/biome-ids.tsv \\\n",
    "local/EnvBroadScaleSoilEnum-pvs-keys-parsed-unique.csv \\\n",
    "local/nmdc-production-biosamples-soil-env_broad_scale.tsv \\\n",
    "local/ncbi-mims-soil-biosamples-env_broad_scale-annotated.tsv \\\n",
    "local/goldData_biosamples-inferred-soil-env_broad_scale-counts.tsv\n",
    "\t$(RUN) python external_metadata_awareness/extract_value_set_evidence.py \\\n",
    "\t\t--config $< \\\n",
    "\t\t--downsample-uncounted \\\n",
    "\t\t--output-file $@\n",
    "```\n",
    "\n"
   ],
   "id": "e98ee08462e02a05"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "```config/soil-env_broad_scale-evidence-config.yaml``` from the same unmerged branch:\n",
    "\n",
    "```yaml\n",
    "- filename: local/biome-ids.tsv\n",
    "  output_prefix: all_biomes_oak\n",
    "  header: false\n",
    "  data_column_number: 1\n",
    "- filename: local/EnvBroadScaleSoilEnum-pvs-keys-parsed-unique.csv\n",
    "  output_prefix: historical_permissible_values\n",
    "  header: true\n",
    "  data_column_name: normalized_curie\n",
    "- filename: local/nmdc-production-biosamples-soil-env_broad_scale.tsv\n",
    "  output_prefix: NMDC_soil\n",
    "  header: false\n",
    "  data_column_number: 1\n",
    "  count_column_number: 2\n",
    "- filename: local/ncbi-mims-soil-biosamples-env_broad_scale-annotated.tsv\n",
    "  output_prefix: NCBI_mims_soil_trusting_CURIe\n",
    "  header: true\n",
    "  data_column_name: normalized_curie\n",
    "  count_column_name: count\n",
    "- filename: local/ncbi-mims-soil-biosamples-env_broad_scale-annotated.tsv\n",
    "  output_prefix: NCBI_mims_soil_trusting_labels\n",
    "  header: true\n",
    "  data_column_name: matched_id\n",
    "  count_column_name: count\n",
    "- filename: local/goldData_biosamples-inferred-soil-env_broad_scale-counts.tsv\n",
    "  output_prefix: GOLD_env_terr_soil\n",
    "  header: false\n",
    "  data_column_number: 1\n",
    "  count_column_number: 2\n",
    "```"
   ],
   "id": "b9aa0126f0fecc23"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Determine the filenames and target directory\n",
    "ncbi_compressed_filename = urlparse(ncbi_duckdb_url).path.split('/')[-1]\n",
    "ncbi_filename = os.path.splitext(ncbi_compressed_filename)[0]\n",
    "target_dir = os.path.join(\"..\", \"..\")  # Two levels up"
   ],
   "id": "6b934c793818f5bc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Fetch the contents from the URL and save compressed file in target directory\n",
    "ncbi_response = requests.get(ncbi_duckdb_url)\n",
    "ncbi_compressed_file_path = os.path.join(target_dir, ncbi_compressed_filename)\n",
    "with open(ncbi_compressed_file_path, \"wb\") as f:\n",
    "    f.write(ncbi_response.content)\n",
    "    \n",
    "# ~ 2 minutes @ 250 Mbps"
   ],
   "id": "8c6147ebd9dcd69d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Unzip the compressed file and save the extracted file in target directory\n",
    "ncbi_uncompressed_file_path = os.path.join(target_dir, ncbi_filename)\n",
    "with gzip.open(ncbi_compressed_file_path, \"rb\") as f_in:\n",
    "    with open(ncbi_uncompressed_file_path, \"wb\") as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "        \n",
    "# ~ 1 minute"
   ],
   "id": "a65971b89486ffe8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ncbi_conn = duckdb.connect(database=ncbi_uncompressed_file_path, read_only=True)",
   "id": "a3c0ec1db5a4f847",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "envo_adapter = get_adapter(envo_adapter_string)",
   "id": "2f5ec46c2396082c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "biome_descendants = get_curie_descendants_label_dict(BIOME_CURIE, [IS_A], envo_adapter)",
   "id": "67808c8134cd1f9a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "biome_descendants_lod = curie_descendants_label_dict_to_lod(biome_descendants)",
   "id": "97f53975986137a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "biome_descendants_frame = curie_descendants_label_lod_to_df(biome_descendants_lod)",
   "id": "6ff81c86ce7c4064",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "biome_descendants_frame",
   "id": "7d9f5e51cb21844",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Use `biome_descendants_frame` as an approximation of `local/biome-ids.tsv`**",
   "id": "4c418e6f7c75da52"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sv = get_schemaview_from_source(previous_submission_schema_url)",
   "id": "3f73cf88902b5ca6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "soil_env_broad_scale_enum = sv.get_enum(ENV_BROAD_SCALE_ENUM)\n",
    "soil_env_broad_scale_pvs_keys = list(soil_env_broad_scale_enum.permissible_values.keys())"
   ],
   "id": "c6355aee16d6a340",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "initially_parsed_soil_env_broad_scale_pvs = parse_hierarchically_underscored_strings(soil_env_broad_scale_pvs_keys)",
   "id": "f0631ea631f6c807",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "deduped_soil_env_broad_scale_pvs = dedupe_underscoreless_pvs(initially_parsed_soil_env_broad_scale_pvs)",
   "id": "4f9efbd6e1e2c834",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "pv_validation_results = validate_curie_label_list_dict(deduped_soil_env_broad_scale_pvs, envo_adapter, print_flag=True)",
   "id": "eb81442383577618",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "pv_validation_results",
   "id": "aa0d1bdc45bb3f3b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Use `pv_validation_results['valids']` as an approximation of `local/EnvBroadScaleSoilEnum-pvs-keys-parsed-unique.csv`**",
   "id": "cd05c55ab2a84b0d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "all_biosamples = get_docs_from_nmdc_collection(NMDC_RUNTIME_BASE_URL,\n",
    "                                               BIOSAMPLE_SET_COLLECTION)  # Example with stop_after"
   ],
   "id": "53d8c3995e597527",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "all_studies = get_docs_from_nmdc_collection(NMDC_RUNTIME_BASE_URL, STUDY_SET_COLLECTION)  # Example with stop_after",
   "id": "95a21f388c30f596",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "env_pacakge_overrides = tsv_to_dict_of_dicts(env_package_override_file, 'id')",
   "id": "c8f6e276e7e29c15",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# env_pacakge_overrides\n",
    "# todo or show as frame\n",
    "# todo include some other columns for context?"
   ],
   "id": "d3bd77c4549f2db3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "biosample_contexts_lod = biosamples_lod_context_extractor(all_biosamples, envo_adapter,\n",
    "                                                          env_pacakge_overrides=env_pacakge_overrides)"
   ],
   "id": "cdd0699ed7bfa22d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "biosample_contexts_frame = pd.DataFrame(biosample_contexts_lod)",
   "id": "4b93dbbb4912a825",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# print a value count for the normalized_env_package column\n",
    "print(\"Value counts for normalized_env_package column:\")\n",
    "print(biosample_contexts_frame['normalized_env_package'].value_counts(dropna=False))"
   ],
   "id": "f7cc947dcc620637",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "package_predictions = predict_from_normalized_env_packages(biosample_contexts_frame, envo_adapter)",
   "id": "c09274a258eaa131",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "biosample_contexts_frame['predicted_env_package'] = package_predictions",
   "id": "8db3619186f07681",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**filter and count, then use `biosample_contexts_frame` as an approximation of `local/nmdc-production-biosamples-soil-env_broad_scale.tsv`**",
   "id": "38d09a3bb6bfb3c3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Filter and count values, then convert to DataFrame\n",
    "soil_counts_df = biosample_contexts_frame[biosample_contexts_frame['predicted_env_package'] == 'soil'][\n",
    "    'env_broad_scale_id'].value_counts().reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "soil_counts_df.columns = ['curie', 'count']\n"
   ],
   "id": "b84c1bda6672c32d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "soil_counts_df",
   "id": "2e0e2a3d08081e40",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ncbi_frame = ncbi_conn.execute(ncbi_query).fetchdf()",
   "id": "5789ecb82c53fd3a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ncbi_frame.insert(0, 'serial_number', range(1, len(ncbi_frame) + 1))",
   "id": "dd689a5d175c9fd9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# includes env broad scale values with counts of one... useful for discovering drag-down submissions?",
   "id": "c33fd385877863f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ncbi_frame['content_list'] = ncbi_frame['content'].str.split('|')",
   "id": "a2492565d3934896",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ncbi_frame['content_count'] = ncbi_frame['content_list'].apply(len)",
   "id": "20aa3fa342102bef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ncbi_frame.shape",
   "id": "34682934e6d4b38d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ncbi_frame = ncbi_frame.explode('content_list').reset_index(drop=True)",
   "id": "d708e65b1ef28344",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ncbi_frame.shape",
   "id": "3feabb64849b8402",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# how many content_list strings contain envo multiple times now?",
   "id": "a76934c8555f4923",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ncbi_frame['envo_count'] = ncbi_frame['content_list'].str.lower().str.count(\"envo\")",
   "id": "ec3f672a6982e836",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ncbi_frame['envo_count'].value_counts()",
   "id": "927b2041141d82ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "doesn't account for multiple label strings delimited with something other than '|'",
   "id": "36d4d1c696c1a5d5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ncbi_frame[['extracted_label', 'extracted_curie']] = ncbi_frame['content_list'].apply(parse_curie_label)",
   "id": "393d2f52a7b7155c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "parse_failures = ncbi_frame[(ncbi_frame['envo_count'] > 0) & (ncbi_frame['extracted_curie'].isna() | (ncbi_frame['extracted_curie'] == ''))]\n",
   "id": "2b310df2655ee870",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "parse_failures",
   "id": "f0ec687bf5e3843c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ncbi_frame['real_label'] = ncbi_frame['extracted_curie'].apply(envo_adapter.label)",
   "id": "6be66d60a992144b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Apply the function to each row in the 'label' column\n",
    "ncbi_frame['longest_annotation_curie'] = ncbi_frame['extracted_label'].apply(lambda x: get_longest_annotation_curie(x, envo_adapter))\n"
   ],
   "id": "8bc7d8d00aeab56",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ncbi_frame['longest_annotation_label'] = ncbi_frame['longest_annotation_curie'].apply(envo_adapter.label)",
   "id": "38e6a45ffdafc058",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ncbi_frame",
   "id": "d393c31729bdfb34",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Use `ncbi_frame` as an approximation of `local/ncbi-mims-soil-biosamples-env_broad_scale-annotated.tsv`**",
   "id": "a34f812b53b3b945"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "gold_biosamples_frame = pd.read_excel(gold_data_url, sheet_name=BIOSAMPLES_SHEET)\n",
    "# 2 minutes"
   ],
   "id": "a66b83e1abdbe5c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "gold_biosamples_frame['BIOSAMPLE ECOSYSTEM PATH ID'] = gold_biosamples_frame['BIOSAMPLE ECOSYSTEM PATH ID'].fillna(\n",
    "    0).astype(int)\n"
   ],
   "id": "e56753c932a76fc9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# gold_biosamples_frame['BIOSAMPLE ECOSYSTEM PATH ID'] = gold_biosamples_frame['BIOSAMPLE ECOSYSTEM PATH ID'].astype(int)",
   "id": "42229a18c7aa03f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "gold_biosamples_frame.columns",
   "id": "eee1b6cfe38f3d40",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "gold_biosamples_frame.shape",
   "id": "faf4b207e69846d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "gold_biosamples_frame",
   "id": "6a4bc761aa25612e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Determine the filenames and target directory\n",
    "goldterms_compressed_filename = urlparse(goldterms_semsql_url).path.split('/')[-1]\n",
    "goldterms_filename = os.path.splitext(goldterms_compressed_filename)[0]\n",
    "target_dir = os.path.join(\"..\", \"..\")  # Two levels up\n",
    "\n",
    "# Print to confirm the filenames\n",
    "print(goldterms_filename)"
   ],
   "id": "900a1ccb2e55da85",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Fetch the contents from the URL and save compressed file in target directory\n",
    "goldterms_response = requests.get(goldterms_semsql_url)\n",
    "goldterms_compressed_file_path = os.path.join(target_dir, goldterms_compressed_filename)\n",
    "with open(goldterms_compressed_file_path, \"wb\") as f:\n",
    "    f.write(goldterms_response.content)"
   ],
   "id": "ba5fb12553bf4825",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Unzip the compressed file and save the extracted file in target directory\n",
    "goldterms_uncompressed_file_path = os.path.join(target_dir, goldterms_filename)\n",
    "with gzip.open(goldterms_compressed_file_path, \"rb\") as f_in:\n",
    "    with open(goldterms_uncompressed_file_path, \"wb\") as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)"
   ],
   "id": "f60c064a587dd407",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "goldterms_conn = sqlite3.connect(goldterms_uncompressed_file_path)",
   "id": "1f663ead96b88e75",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "goldterms_soil_subjects = pd.read_sql_query(goldterms_soil_subclass_query, goldterms_conn)",
   "id": "bcedfe9ddb7254fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "goldterms_soil_subjects['path_id'] = goldterms_soil_subjects['subject'].str.extract(r'GOLDTERMS:(\\d+)')",
   "id": "82a240e419eb635d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "goldterms_soil_subjects",
   "id": "8e90c8dfb6b820b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "soil_path_ids = goldterms_soil_subjects['path_id'].dropna().unique().tolist()\n",
    "soil_path_ids = [int(id) for id in soil_path_ids]\n"
   ],
   "id": "9e70094c5c28b5ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "gold_soil_biosamples_frame = gold_biosamples_frame[\n",
    "    gold_biosamples_frame['BIOSAMPLE ECOSYSTEM PATH ID'].isin(soil_path_ids)]\n"
   ],
   "id": "d294ca1680c31685",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "gold_soil_biosamples_frame",
   "id": "ea40acff447df533",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "goldterms_mixs_broad_frame = pd.read_sql_query(goldterms_envo_query, goldterms_conn)",
   "id": "4b4ed63d71bf1e65",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "goldterms_mixs_broad_frame['mixs_broad_label'] = goldterms_mixs_broad_frame['object'].apply(envo_adapter.label)",
   "id": "e736fe9475ea3d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "goldterms_mixs_broad_frame['path_id'] = goldterms_mixs_broad_frame['subject'].str.extract(r'GOLDTERMS:(\\d+)')",
   "id": "ce93f9d692231ed8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "goldterms_mixs_broad_frame",
   "id": "f0e775fd25205bcd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Fill NaN values in 'BIOSAMPLE ECOSYSTEM PATH ID' with 0 and convert to int\n",
    "gold_soil_biosamples_frame['BIOSAMPLE ECOSYSTEM PATH ID'] = gold_soil_biosamples_frame['BIOSAMPLE ECOSYSTEM PATH ID'].fillna(0).astype(int)\n",
    "\n",
    "# Drop rows with NaN in 'path_id' in goldterms_mixs_broad_frame\n",
    "goldterms_mixs_broad_frame = goldterms_mixs_broad_frame.dropna(subset=['path_id'])\n",
    "\n",
    "# Convert 'path_id' to int\n",
    "goldterms_mixs_broad_frame['path_id'] = goldterms_mixs_broad_frame['path_id'].astype(int)\n",
    "\n",
    "# Perform the left merge\n",
    "gold_soil_biosamples_inferred_broad = gold_soil_biosamples_frame.merge(\n",
    "    goldterms_mixs_broad_frame,\n",
    "    left_on='BIOSAMPLE ECOSYSTEM PATH ID',\n",
    "    right_on='path_id',\n",
    "    how='left'\n",
    ")\n"
   ],
   "id": "7a3f4269e0736579",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "gold_soil_biosamples_inferred_broad",
   "id": "27e616ea4cbd27ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Use `gold_soil_biosamples_inferred_broad` as an approximation of `local/goldData_biosamples-inferred-soil-env_broad_scale-counts.tsv`**",
   "id": "7e6055a76a17daf0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "559b36b44158c43a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
